{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hi :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RECORDING_SESSION_LABEL</th>\n",
       "      <th>trial</th>\n",
       "      <th>IA_ID</th>\n",
       "      <th>item</th>\n",
       "      <th>list</th>\n",
       "      <th>IA_LABEL</th>\n",
       "      <th>wordlength</th>\n",
       "      <th>condition</th>\n",
       "      <th>is_critical</th>\n",
       "      <th>is_spill1</th>\n",
       "      <th>...</th>\n",
       "      <th>duration_firstpass</th>\n",
       "      <th>duration_firstfixation</th>\n",
       "      <th>fix_count</th>\n",
       "      <th>avg_pupil</th>\n",
       "      <th>IA_REGRESSION_IN_COUNT</th>\n",
       "      <th>IA_REGRESSION_OUT_COUNT</th>\n",
       "      <th>saccade_length</th>\n",
       "      <th>saccade_duration</th>\n",
       "      <th>go_past_time</th>\n",
       "      <th>sentenceCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10m23r2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Viel</td>\n",
       "      <td>4</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>236</td>\n",
       "      <td>236</td>\n",
       "      <td>1</td>\n",
       "      <td>1408.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72.376792</td>\n",
       "      <td>16</td>\n",
       "      <td>236</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10m23r2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Geld</td>\n",
       "      <td>4</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>424</td>\n",
       "      <td>264</td>\n",
       "      <td>3</td>\n",
       "      <td>1379.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>71.519648</td>\n",
       "      <td>160</td>\n",
       "      <td>424</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10m23r2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>wurde</td>\n",
       "      <td>5</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10m23r2</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>investiert,</td>\n",
       "      <td>11</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>420</td>\n",
       "      <td>268</td>\n",
       "      <td>3</td>\n",
       "      <td>1290.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>65.401223</td>\n",
       "      <td>12</td>\n",
       "      <td>420</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10m23r2</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>bevor</td>\n",
       "      <td>5</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>296</td>\n",
       "      <td>296</td>\n",
       "      <td>2</td>\n",
       "      <td>1242.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>42.311819</td>\n",
       "      <td>12</td>\n",
       "      <td>296</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11171</th>\n",
       "      <td>9m23r1</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>Verteidigung</td>\n",
       "      <td>12</td>\n",
       "      <td>filler</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>168</td>\n",
       "      <td>168</td>\n",
       "      <td>2</td>\n",
       "      <td>497.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>91.810675</td>\n",
       "      <td>20</td>\n",
       "      <td>168</td>\n",
       "      <td>filler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11172</th>\n",
       "      <td>9m23r1</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>Europas</td>\n",
       "      <td>7</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>1</td>\n",
       "      <td>493.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>119.788355</td>\n",
       "      <td>24</td>\n",
       "      <td>224</td>\n",
       "      <td>filler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11173</th>\n",
       "      <td>9m23r1</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>erhöht</td>\n",
       "      <td>6</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "      <td>1</td>\n",
       "      <td>472.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>95.117033</td>\n",
       "      <td>20</td>\n",
       "      <td>332</td>\n",
       "      <td>filler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11174</th>\n",
       "      <td>9m23r1</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>werden</td>\n",
       "      <td>6</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>244</td>\n",
       "      <td>244</td>\n",
       "      <td>1</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.649126</td>\n",
       "      <td>16</td>\n",
       "      <td>244</td>\n",
       "      <td>filler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11175</th>\n",
       "      <td>9m23r1</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>sollten.</td>\n",
       "      <td>8</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>495.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1164</td>\n",
       "      <td>filler</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11176 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      RECORDING_SESSION_LABEL  trial  IA_ID  item  list      IA_LABEL  \\\n",
       "0                     10m23r2     12      1     1     2          Viel   \n",
       "1                     10m23r2     12      2     1     2          Geld   \n",
       "2                     10m23r2     12      3     1     2         wurde   \n",
       "3                     10m23r2     12      4     1     2   investiert,   \n",
       "4                     10m23r2     12      5     1     2         bevor   \n",
       "...                       ...    ...    ...   ...   ...           ...   \n",
       "11171                  9m23r1     10     12    28     1  Verteidigung   \n",
       "11172                  9m23r1     10     13    28     1       Europas   \n",
       "11173                  9m23r1     10     14    28     1        erhöht   \n",
       "11174                  9m23r1     10     15    28     1        werden   \n",
       "11175                  9m23r1     10     16    28     1      sollten.   \n",
       "\n",
       "       wordlength condition  is_critical  is_spill1  ...  duration_firstpass  \\\n",
       "0               4      none            0          0  ...                 236   \n",
       "1               4      none            0          0  ...                 424   \n",
       "2               5      none            0          0  ...                   0   \n",
       "3              11      none            0          0  ...                 420   \n",
       "4               5      none            0          0  ...                 296   \n",
       "...           ...       ...          ...        ...  ...                 ...   \n",
       "11171          12    filler            0          0  ...                 168   \n",
       "11172           7      none            0          0  ...                 224   \n",
       "11173           6      none            0          0  ...                 332   \n",
       "11174           6      none            0          0  ...                 244   \n",
       "11175           8      none            0          0  ...                 160   \n",
       "\n",
       "       duration_firstfixation  fix_count    avg_pupil  IA_REGRESSION_IN_COUNT  \\\n",
       "0                         236          1  1408.000000                       0   \n",
       "1                         264          3  1379.333333                       2   \n",
       "2                           0          0     0.000000                       0   \n",
       "3                         268          3  1290.000000                       1   \n",
       "4                         296          2  1242.500000                       1   \n",
       "...                       ...        ...          ...                     ...   \n",
       "11171                     168          2   497.000000                       0   \n",
       "11172                     224          1   493.000000                       0   \n",
       "11173                     332          1   472.000000                       0   \n",
       "11174                     244          1   477.000000                       0   \n",
       "11175                     160          1   495.000000                       0   \n",
       "\n",
       "       IA_REGRESSION_OUT_COUNT  saccade_length  saccade_duration  \\\n",
       "0                            0       72.376792                16   \n",
       "1                            0       71.519648               160   \n",
       "2                            1        0.000000                 0   \n",
       "3                            0       65.401223                12   \n",
       "4                            0       42.311819                12   \n",
       "...                        ...             ...               ...   \n",
       "11171                        0       91.810675                20   \n",
       "11172                        0      119.788355                24   \n",
       "11173                        0       95.117033                20   \n",
       "11174                        0       58.649126                16   \n",
       "11175                        1        0.000000                 0   \n",
       "\n",
       "       go_past_time  sentenceCondition  \n",
       "0               236            control  \n",
       "1               424            control  \n",
       "2                 0            control  \n",
       "3               420            control  \n",
       "4               296            control  \n",
       "...             ...                ...  \n",
       "11171           168             filler  \n",
       "11172           224             filler  \n",
       "11173           332             filler  \n",
       "11174           244             filler  \n",
       "11175          1164             filler  \n",
       "\n",
       "[11176 rows x 29 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import product # used for hyperparameter grid search, unused if not doing hyperparameter tuning\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seed = 42 # for reproducibility\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.features.iloc[index].to_numpy()\n",
    "        label = self.labels.iloc[index]\n",
    "        return features, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, batch_size, task=1):\n",
    "    \n",
    "    dataloaders = []\n",
    "    global class_weights\n",
    "\n",
    "    if task == 0 or task == 1: # Known subjects and items\n",
    "        features, labels = data\n",
    "        dataset = CustomDataset(features, labels)\n",
    "        n = len(dataset)\n",
    "        if task == 0:\n",
    "            k = 10 # k-fold cross-validation\n",
    "        elif task == 1:\n",
    "            k = n # leave-one-out cross-validation\n",
    "        fold_size = n // k\n",
    "        folds = []\n",
    "        for i in range(k):\n",
    "            start = i * fold_size\n",
    "            end = (i + 1) * fold_size if i < k - 1 else n\n",
    "            folds.append(torch.utils.data.Subset(dataset, range(start, end)))\n",
    "\n",
    "        for i in range(k):\n",
    "            # splits for cross-validation, validation set = test set (since we're doing k-fold, we won't use a separate test set)\n",
    "            validation_dataset = folds[i]\n",
    "            t = i + 1 if i < k - 1 else 0\n",
    "            test_dataset = folds[t]\n",
    "            train_folds = [folds[j] for j in range(k) if j != i]# and j != t]\n",
    "            train_dataset = torch.utils.data.ConcatDataset(train_folds)\n",
    "\n",
    "            # class weights for weighted cross-entropy loss (to handle class imbalance)\n",
    "            y = torch.tensor([label for _, label in train_dataset], dtype=torch.long)\n",
    "            \n",
    "            class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y.numpy())\n",
    "            class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "            test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "            dataloaders.append((train_dataloader, validation_dataloader, test_dataloader))\n",
    "            #dataloaders.append((train_dataloader, validation_dataloader))\n",
    "\n",
    "        return dataloaders\n",
    "    \n",
    "    elif task == 2: # Held-out subjects, known items\n",
    "        test_items_count = 0\n",
    "        for i, subject in enumerate(data.groups.keys()):\n",
    "            test = data.get_group(subject)\n",
    "            test_items_count += len(test)\n",
    "            train_eval = pd.concat([data.get_group(i) for i in data.groups.keys() if i != subject])\n",
    "            shuffled = train_eval.sample(frac = 1, random_state=seed) # shuffle the data -> wrecked.\n",
    "            \n",
    "            # splitting data into features and labels for dataset creation\n",
    "            test_labels = test[\"condition\"].copy()\n",
    "            test_features = test.copy().drop([\"condition\", \"sentenceCondition\", \"RECORDING_SESSION_LABEL\", \"trial\"], axis=1)\n",
    "            test_dataset = CustomDataset(test_features, test_labels)\n",
    "            \n",
    "            train_eval_labels = shuffled[\"condition\"].copy()\n",
    "            train_eval_features = shuffled.copy().drop([\"condition\", \"sentenceCondition\", \"RECORDING_SESSION_LABEL\", \"trial\"], axis=1)\n",
    "            train_eval_dataset = CustomDataset(train_eval_features, train_eval_labels)\n",
    "\n",
    "\n",
    "            train_eval_split = 0.9\n",
    "            train_size = int(train_eval_split * len(train_eval_dataset))\n",
    "            validation_size = len(train_eval_dataset) - train_size\n",
    "            train_dataset, validation_dataset = torch.utils.data.random_split(train_eval_dataset, [train_size, validation_size])\n",
    "\n",
    "            y = torch.tensor([label for _, label in train_dataset], dtype=torch.long)\n",
    "\n",
    "            class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y.numpy())\n",
    "            class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "            test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "            dataloaders.append((train_dataloader, validation_dataloader, test_dataloader))\n",
    "        return dataloaders\n",
    "    \n",
    "    elif task == 3: # Held-out items, known subjects\n",
    "        test_items_count = 0\n",
    "        for i, item in enumerate(data.groups.keys()):\n",
    "            test = data.get_group(item)\n",
    "            test_items_count += len(test)\n",
    "            train_eval = pd.concat([data.get_group(i) for i in data.groups.keys() if i != item])\n",
    "            shuffled = train_eval.sample(frac = 1, random_state=seed) # shuffle the data -> wrecked.\n",
    "            \n",
    "            # splitting data into features and labels for dataset creation\n",
    "            test_labels = test[\"condition\"].copy()\n",
    "            test_features = test.copy().drop([\"condition\", \"sentenceCondition\", \"RECORDING_SESSION_LABEL\", \"trial\"], axis=1)\n",
    "            test_dataset = CustomDataset(test_features, test_labels)\n",
    "            \n",
    "            train_eval_labels = shuffled[\"condition\"].copy()\n",
    "            train_eval_features = shuffled.copy().drop([\"condition\", \"sentenceCondition\", \"RECORDING_SESSION_LABEL\", \"trial\"], axis=1)\n",
    "            train_eval_dataset = CustomDataset(train_eval_features, train_eval_labels)\n",
    "\n",
    "\n",
    "            train_eval_split = 0.9\n",
    "            train_size = int(train_eval_split * len(train_eval_dataset))\n",
    "            validation_size = len(train_eval_dataset) - train_size\n",
    "            train_dataset, validation_dataset = torch.utils.data.random_split(train_eval_dataset, [train_size, validation_size])\n",
    "\n",
    "            y = torch.tensor([label for _, label in train_dataset], dtype=torch.long)\n",
    "\n",
    "            class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y.numpy())\n",
    "            class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "            test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "            dataloaders.append((train_dataloader, validation_dataloader, test_dataloader))\n",
    "        return dataloaders\n",
    "    else:\n",
    "        raise ValueError(\"Task argument must be either 1, 2, or 3\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_split_data(data, batch_size=32, task=1):\n",
    "    \n",
    "# all tasks\n",
    "    \n",
    "    data_copy = data.loc[data['is_critical'] == 1].copy()\n",
    "    dropped = data_copy.drop(['composite', 'LF', 'HF', \"IA_ID\", \"item\", \"list\", \"IA_LABEL\", \"wordlength\", \"is_critical\", \n",
    "                'is_spill1', 'is_spill2', 'is_spill3', 'filler', 'function_word', 'other_filler'], axis=1)\n",
    "    print(\"Original dataset size: \", len(data_copy))\n",
    "\n",
    "    # normalizing input features beforehand, increased performance vs adding batchnorm layer to model\n",
    "    temp = dropped[['fixation_duration',\n",
    "        'duration_firstpass', 'duration_firstfixation', 'fix_count',\n",
    "        'avg_pupil', 'IA_REGRESSION_IN_COUNT', 'IA_REGRESSION_OUT_COUNT',\n",
    "        'saccade_length', 'saccade_duration', 'go_past_time']]\n",
    "    temp = (temp - temp.mean()) / temp.std()\n",
    "    dropped[['fixation_duration',\n",
    "        'duration_firstpass', 'duration_firstfixation', 'fix_count',\n",
    "        'avg_pupil', 'IA_REGRESSION_IN_COUNT', 'IA_REGRESSION_OUT_COUNT',\n",
    "        'saccade_length', 'saccade_duration', 'go_past_time']] = temp\n",
    "    normalized = dropped\n",
    "    # mapping condition and sentenceCondition to 0 and 1 for critical word classification\n",
    "    normalized[[\"condition\", \"sentenceCondition\"]] = normalized[[\"condition\", \"sentenceCondition\"]].map(lambda x: x.replace(\"none\", \"0\"))\n",
    "    normalized[[\"condition\", \"sentenceCondition\"]] = normalized[[\"condition\", \"sentenceCondition\"]].map(lambda x: x.replace(\"control\", \"0\"))\n",
    "    normalized[[\"condition\", \"sentenceCondition\"]] = normalized[[\"condition\", \"sentenceCondition\"]].map(lambda x: x.replace(\"pseudo\", \"1\"))\n",
    "    normalized[[\"condition\", \"sentenceCondition\"]] = normalized[[\"condition\", \"sentenceCondition\"]].map(lambda x: x.replace(\"filler\", \"0\"))\n",
    "    normalized[[\"condition\", \"sentenceCondition\"]] = normalized[[\"condition\", \"sentenceCondition\"]].astype(int)\n",
    "    mapped = normalized\n",
    "\n",
    "# task 1 specific steps\n",
    "    if task == 0 or task == 1: # Known subjects and items\n",
    "        shuffled = mapped.sample(frac = 1, random_state=seed) # shuffle the data -> wrecked.\n",
    "        # splitting data into features and labels for dataset creation\n",
    "        labels = shuffled[\"condition\"].copy()\n",
    "        features = shuffled.copy().drop([\"condition\", \"sentenceCondition\", \"RECORDING_SESSION_LABEL\", \"trial\"], axis=1)\n",
    "        print(\"Preprocessed dataset size: \", len(features))\n",
    "        data = (features, labels)\n",
    "        return split_data(data, batch_size, task)\n",
    "    \n",
    "    elif task == 2: # Held-out subjects, known items # TODO\n",
    "        subjects = mapped.groupby('RECORDING_SESSION_LABEL')\n",
    "        print(len(subjects))\n",
    "        return split_data(subjects, batch_size, task)\n",
    "    elif task == 3: # Held-out items, known subjects # TODO\n",
    "        items = mapped.groupby('trial')\n",
    "        print(len(items))\n",
    "        return split_data(items, batch_size, task)\n",
    "    else:\n",
    "        raise ValueError(\"Task argument must be either 1, 2, or 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(model, dataloader, optimizer, training=\"train\"):\n",
    "   \n",
    "    loss_function = torch.nn.BCEWithLogitsLoss()#weight=class_weights.to(device))\n",
    "\n",
    "    if training == \"train\":\n",
    "        model.train()\n",
    "    elif training == \"validation\":\n",
    "        model.eval()\n",
    "    elif training == \"test\":\n",
    "        model.eval()\n",
    "    else:\n",
    "        raise ValueError(\"training argument must be either 'train', 'validation' or 'test'\")\n",
    "        \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    cumulative_loss = 0\n",
    "    prediction_list = []\n",
    "    label_list = []\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    for sample in dataloader:\n",
    "   \n",
    "        data, targets = sample[0].float().to(device), sample[1].type(torch.LongTensor).to(device)\n",
    "        output = model(data)\n",
    "        loss_value = loss_function(output, targets.unsqueeze(1).float())\n",
    "        cumulative_loss += loss_value.item()\n",
    "\n",
    "        if training == \"train\":\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        predictions = [round(x) for x in sigmoid(output).to('cpu').detach().squeeze(1).numpy().tolist()]#.argmax(axis=1)\n",
    "        target_labels = targets.to('cpu').detach().numpy()\n",
    "        total += len(predictions)\n",
    "        correct += accuracy_score(target_labels, predictions, normalize=False)\n",
    "        prediction_list.extend(predictions)\n",
    "        label_list.extend(target_labels)\n",
    "    if training == \"test\":\n",
    "        return label_list, prediction_list\n",
    "    f1 = f1_score(label_list, prediction_list)\n",
    "    accuracy = accuracy_score(label_list, prediction_list)\n",
    "    confusion = confusion_matrix(label_list, prediction_list)\n",
    "\n",
    "    return cumulative_loss, accuracy, f1, confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TuneableModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, layer_size, dropout_rate, n_layers):\n",
    "        super(TuneableModel, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.input_layer = torch.nn.LazyLinear(layer_size)\n",
    "        self.linear2 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear3 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear4 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear5 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear6 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear7 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear8 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear9 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear10 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.output_layer = torch.nn.Linear(layer_size, 1)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.activation = torch.nn.LeakyReLU()\n",
    "        self.batchnorm = torch.nn.BatchNorm1d(layer_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        #x = self.batchnorm(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        if self.n_layers > 1:\n",
    "            x = self.linear2(x)\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout(x)\n",
    "            if self.n_layers > 2:\n",
    "                x = self.linear3(x)\n",
    "                x = self.activation(x)\n",
    "                x = self.dropout(x)\n",
    "                if self.n_layers > 3:\n",
    "                    x = self.linear4(x)\n",
    "                    x = self.activation(x)\n",
    "                    x = self.dropout(x)\n",
    "                    if self.n_layers > 4:\n",
    "                        x = self.linear5(x)\n",
    "                        x = self.activation(x)\n",
    "                        x = self.dropout(x)\n",
    "                        if self.n_layers > 5:\n",
    "                            x = self.linear6(x)\n",
    "                            x = self.activation(x)\n",
    "                            x = self.dropout(x)\n",
    "                            if self.n_layers > 6:\n",
    "                                x = self.linear7(x)\n",
    "                                x = self.activation(x)\n",
    "                                x = self.dropout(x)\n",
    "                                if self.n_layers > 7:\n",
    "                                    x = self.linear8(x)\n",
    "                                    x = self.activation(x)\n",
    "                                    x = self.dropout(x)\n",
    "                                    if self.n_layers > 8:\n",
    "                                        x = self.linear9(x)\n",
    "                                        x = self.activation(x)\n",
    "                                        x = self.dropout(x)\n",
    "                                        if self.n_layers > 9:\n",
    "                                            x = self.linear10(x)\n",
    "                                            x = self.activation(x)\n",
    "                                            x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        #x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training sample\n",
    "def evaluate(data, parameters, task):\n",
    "    assert task in [0, 1, 2, 3], \"Task argument must be either 1, 2 or 3\"\n",
    "    \n",
    "    dropout, hidden_size, learning_rate, batch_size, n_hidden, beta_1, beta_2 = parameters\n",
    "\n",
    "    max_epochs = 1000\n",
    "\n",
    "    dataloaders = preprocess_and_split_data(data, batch_size, task)\n",
    "\n",
    "    input_size = 10 # number of features :( -> this is hardcoded for now, try to get it from the dataset\n",
    "    best_epochs = []\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    torch.manual_seed(seed)\n",
    "    model = TuneableModel(input_size, hidden_size, dropout, n_hidden)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(beta_1, beta_2), weight_decay=1e-2)\n",
    "\n",
    "\n",
    "    \n",
    "    for i, dataloader in tqdm(enumerate(dataloaders)):\n",
    "        max_patience = 10 if i < 35 else 2\n",
    "        last_loss = 1000000\n",
    "        best_epoch = 0\n",
    "        PATH = f\"model_{i}.pt\"\n",
    "        train_dataloader, validation_dataloader, test_dataloader = dataloader[0], dataloader[1], dataloader[2]\n",
    "        for epoch in range(max_epochs):\n",
    "            # training\n",
    "            train_loss, train_accuracy, train_f1, train_confusion = train_test(model, train_dataloader, optimizer, training=\"train\")\n",
    "            train_loss, train_accuracy, train_f1 = round(train_loss, 2), round(train_accuracy, 4), round(train_f1, 2)\n",
    "            # validation at end of epoch\n",
    "            validation_loss, validation_accuracy, validation_f1, validation_confusion = train_test(model, validation_dataloader, optimizer, training=\"validation\")\n",
    "            validation_loss, validation_accuracy, validation_f1 = round(validation_loss, 2), round(validation_accuracy, 4), round(validation_f1, 2)\n",
    "            if validation_loss < last_loss:\n",
    "                last_loss = validation_loss\n",
    "                best_epoch = epoch\n",
    "                current_patience = 0\n",
    "            else:\n",
    "                if current_patience == 0:\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': last_loss,\n",
    "                        }, PATH)\n",
    "                current_patience += 1\n",
    "            if current_patience == max_patience:\n",
    "                break   \n",
    "            # if epoch % 100 == 0 and epoch != 0:\n",
    "            #     print(f\"Epoch {epoch}: Train loss: {train_loss}, Train accuracy: {train_accuracy}, Train f1: {train_f1}\")\n",
    "            #     print(f\"Epoch {epoch}: Validation loss: {validation_loss}, Validation accuracy: {validation_accuracy}, Validation f1: {validation_f1}\")\n",
    "\n",
    "        # Testing once patience is reached\n",
    "        torch.manual_seed(seed)\n",
    "        model = TuneableModel(input_size, hidden_size, dropout, n_hidden)\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.99, 0.99), weight_decay=1e-4)\n",
    "        checkpoint = torch.load(PATH)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        prediction_list, label_list = train_test(model, test_dataloader, optimizer, training=\"test\")\n",
    "        predictions.extend(prediction_list)\n",
    "        labels.extend(label_list)\n",
    "        best_epochs.append(best_epoch)\n",
    "    print(\"Average training epochs for best model:\", round(np.mean(best_epochs), 1))\n",
    "    print(\"Best epochs:\\n\\t\", best_epochs)\n",
    "    return accuracy_score(labels, predictions), f1_score(labels, predictions), confusion_matrix(labels, predictions)\n",
    "    # print(f\"Average accuracy: {round(np.mean(accuracies), 2)}%\")\n",
    "    # print(f\"Average f1: {round(np.mean(f1s), 2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size:  343\n",
      "Preprocessed dataset size:  343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:01, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[95], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m parameters \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m0.999\u001b[39m, \u001b[38;5;241m0.999\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]:\n\u001b[1;32m---> 50\u001b[0m     accuracy, f1, confusion \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAcc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(accuracy\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mF1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(f1,\u001b[38;5;241m4\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfusion:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, confusion)\n",
      "Cell \u001b[1;32mIn[92], line 31\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(data, parameters, task)\u001b[0m\n\u001b[0;32m     28\u001b[0m train_dataloader, validation_dataloader, test_dataloader \u001b[38;5;241m=\u001b[39m dataloader[\u001b[38;5;241m0\u001b[39m], dataloader[\u001b[38;5;241m1\u001b[39m], dataloader[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_epochs):\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# training\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     train_loss, train_accuracy, train_f1, train_confusion \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     train_loss, train_accuracy, train_f1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(train_loss, \u001b[38;5;241m2\u001b[39m), \u001b[38;5;28mround\u001b[39m(train_accuracy, \u001b[38;5;241m4\u001b[39m), \u001b[38;5;28mround\u001b[39m(train_f1, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# validation at end of epoch\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[90], line 23\u001b[0m, in \u001b[0;36mtrain_test\u001b[1;34m(model, dataloader, optimizer, training)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     22\u001b[0m     data, targets \u001b[38;5;241m=\u001b[39m sample[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device), sample[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mLongTensor)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 23\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m loss_function(output, targets\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m     25\u001b[0m     cumulative_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_value\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[91], line 30\u001b[0m, in \u001b[0;36mTuneableModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 30\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(x)\n\u001b[0;32m     32\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Best parameters:\n",
    "    Patience = 10\n",
    "    @ model_params(\n",
    "        dropout: 0.0\n",
    "        layer size: 500\n",
    "        lr: 0.001\n",
    "        batch_size: 16\n",
    "        n_layers: 6)\n",
    "    @ optimizer_AdamW(\n",
    "        betas=(0.999, 0.999), \n",
    "        weight_decay=1e-2)\n",
    "\n",
    "\n",
    "Performance:\n",
    "    Task 1: Known subjects, known items\n",
    "\n",
    "        Train:Eval:Test - 80:10:10 - 10-fold\n",
    "        Training epochs before convergence:\n",
    "            [5, 4, 0, 1, 0, 3, 10, 0, 0, 0]\n",
    "        Acc: 97.38%\n",
    "        F1: 0.9474\n",
    "        Confusion:\n",
    "            [[253   4]\n",
    "            [  5  81]]\n",
    "\n",
    "        Blind LOOCV: Train:Eval:Test - n-2:1:1\n",
    "        Acc: 99.71%\n",
    "        F1: 0.9942\n",
    "        Confusion:\n",
    "            [[257   0]\n",
    "            [  1  85]]\n",
    "            \n",
    "    Task 2: Left-out subjects, known items, train:eval:test - 90:10:subject\n",
    "        Training epochs before convergence:\n",
    "            [3, 1, 0, 0, 0, 2, 0, 1, 2, 8, 0, 3, 0, 0, 0, 0, 2, 0, 0, 0, 0, 5, 0, 0, 6, 0, 0, 0, 1, 0, 0, 0]\n",
    "        Acc: 98.25%\n",
    "        F1: 0.965\n",
    "            [[255   3]\n",
    "            [  3  82]]\n",
    "\n",
    "    Task 3: Left-out items, known subjects, train:eval:test - 90:10:item\n",
    "        Training epochs before convergence:\n",
    "            [5, 0, 4, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 0, 0, 0, 0, 0, 0, 5]\n",
    "        Acc: 99.13%\n",
    "        F1: 0.983\n",
    "            [[255   0]\n",
    "            [  3  85]]\n",
    "\"\"\"\n",
    "\n",
    "parameters = (0.0, 500, 0.001, 16, 6, 0.999, 0.999)\n",
    "for task in [0, 1, 2, 3]:\n",
    "    accuracy, f1, confusion = evaluate(data, parameters, task)\n",
    "    print(f\"Acc: {round(accuracy*100,2)}%\\nF1: {round(f1,4)}\")\n",
    "    print(\"Confusion:\\n\", confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size:  343\n",
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [01:05,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training epochs for best model: 1.1\n",
      "Best epochs:\n",
      "\t [3, 1, 0, 0, 0, 2, 0, 1, 2, 8, 0, 3, 0, 0, 0, 0, 2, 0, 0, 0, 0, 5, 0, 0, 6, 0, 0, 0, 1, 0, 0, 0]\n",
      "acc: 98.25%\n",
      " f1: 0.965\n",
      "[[255   3]\n",
      " [  3  82]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy, f1, confusion = evaluate(data, parameters, task=2)\n",
    "print(f\"acc: {round(accuracy*100,2)}%\\n f1: {round(f1,3)}\")\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, f1, confusion = evaluate(data=data, parameters=params, task=3)\n",
    "print(f\"acc: {round(accuracy*100,2)}%\\n f1: {round(f1,3)}\")\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_nn ={\n",
    "#     'dropout': [0.5],\n",
    "#     'hidden_size': list(range(500, 501, 100)),\n",
    "#     'learning_rate':[0.01, 0.001, 0.0001, 0.00001],\n",
    "#     'batch_size':[8, 16, 32, 64, 128],\n",
    "#     'n_hidden': list(range(1, 4, 1))\n",
    "# }\n",
    "# parameter_expansion = list(product(*params_nn.values()))\n",
    "# print(len(parameter_expansion))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
