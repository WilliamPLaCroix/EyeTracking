{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hi :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import product # used for hyperparameter grid search, unused if not doing hyperparameter tuning\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seed = 42 # for reproducibility\n",
    "torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed) # why did this break my predictions?!?\n",
    "\n",
    "data = pd.read_csv('data.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    CustomDataset is a class for creating a dataset in PyTorch, inheriting from the PyTorch Dataset class.\n",
    "    This dataset is designed to handle tabular data provided as pandas DataFrames.\n",
    "\n",
    "    Attributes:\n",
    "        features (pd.DataFrame): A DataFrame containing the features of the dataset.\n",
    "        labels (pd.Series or pd.DataFrame): A Series or DataFrame containing the labels of the dataset.\n",
    "    Methods:\n",
    "        __getitem__(self, index): Returns the features and label for a given index.\n",
    "        __len__(self): Returns the total number of samples in the dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, features, labels):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            features (pd.DataFrame): The features of the dataset.\n",
    "            labels (pd.Series or pd.DataFrame): The labels of the dataset.\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            index (int): The index of the item to retrieve.\n",
    "        Returns:\n",
    "            tuple: A tuple containing the features as a numpy array and the label.\n",
    "        \"\"\"\n",
    "        features = self.features.iloc[index].to_numpy()\n",
    "        label = self.labels.iloc[index]\n",
    "        return features, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            int: The total number of samples.\n",
    "        \"\"\"\n",
    "        return len(self.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, batch_size, task, alpha):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "        data (pd.DataFrame or dict): The dataset to split. If `task` is 0 or 1, `data` should be a tuple \n",
    "            of (features, labels) where both are pandas DataFrames. If `task` is 2 or 3, `data` should be \n",
    "            a dictionary where keys are subject or item IDs and values are pandas DataFrames corresponding \n",
    "            to data for each subject/item.\n",
    "        batch_size (int): The size of each batch to be loaded by the DataLoader.\n",
    "        task (int): An integer indicating the type of task to prepare data for. Valid values are:\n",
    "            0 - Known subjects and items with k-fold cross-validation.\n",
    "            1 - Known subjects and items with leave-one-out cross-validation.\n",
    "            2 - Held-out subjects, known items.\n",
    "            3 - Held-out items, known subjects.\n",
    "        alpha (float): A scaling factor used in computing positive class weights for class imbalance handling in BCELoss.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two elements:\n",
    "            - A list of tuples, each containing DataLoader objects for the training, validation, \n",
    "              and testing sets in that order.\n",
    "            - A list of torch.tensor objects representing the positive class weights computed for \n",
    "              each split to address class imbalance.\n",
    "    \"\"\"\n",
    "    \n",
    "    dataloaders = []\n",
    "    pos_weights = []\n",
    "    # global class_weights\n",
    "\n",
    "    if task == 0 or task == 1: # Known subjects and items\n",
    "        features, labels = data\n",
    "        dataset = CustomDataset(features, labels)\n",
    "        n = len(dataset)\n",
    "        if task == 0:\n",
    "            k = 10 # k-fold cross-validation\n",
    "        elif task == 1:\n",
    "            k = n # leave-one-out cross-validation\n",
    "        fold_size = n // k\n",
    "        folds = []\n",
    "        for i in range(k):\n",
    "            start = i * fold_size\n",
    "            end = (i + 1) * fold_size if i < k - 1 else n\n",
    "            folds.append(torch.utils.data.Subset(dataset, range(start, end)))\n",
    "\n",
    "        for i in range(k):\n",
    "            # splits for cross-validation, validation set = test set (since we're doing k-fold, we won't use a separate test set)\n",
    "            validation_dataset = folds[i]\n",
    "            t = i + 1 if i < k - 1 else 0\n",
    "            test_dataset = folds[t]\n",
    "            train_folds = [folds[j] for j in range(k) if j != i]# and j != t]\n",
    "            train_dataset = torch.utils.data.ConcatDataset(train_folds)\n",
    "\n",
    "            # class weights for weighted cross-entropy loss (to handle class imbalance)\n",
    "            y = torch.tensor([label for _, label in train_dataset], dtype=torch.long)\n",
    "            sum = y.sum().item()\n",
    "            weight = alpha*(len(train_dataset)-sum) / sum\n",
    "            pos_weights.append(torch.tensor(weight, dtype=torch.float))\n",
    "\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "            test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "            dataloaders.append((train_dataloader, validation_dataloader, test_dataloader))\n",
    "            #dataloaders.append((train_dataloader, validation_dataloader))\n",
    "\n",
    "        return dataloaders, pos_weights\n",
    "    \n",
    "    elif task == 2: # Held-out subjects, known items\n",
    "        test_items_count = 0\n",
    "        subject_ids = list(data.groups.keys())\n",
    "\n",
    "        for i in range(0, len(subject_ids), 2):\n",
    "            subject_indexes = [i, i+1]\n",
    "            subjects = [subject_ids[subject_index] for subject_index in subject_indexes]\n",
    "\n",
    "            others = [subject for j, subject in enumerate(subject_ids) if j != i and j != i+1]\n",
    "\n",
    "            test = pd.concat([data.get_group(subject) for subject in subjects])\n",
    "            test_items_count += len(test)\n",
    "            train_eval = pd.concat([data.get_group(subject) for subject in others])\n",
    "            shuffled = train_eval.sample(frac = 1, random_state=seed) # shuffle the data -> wrecked.\n",
    "            \n",
    "            # splitting data into features and labels for dataset creation\n",
    "            test_labels = test[\"condition\"].copy()\n",
    "            test_features = test.copy().drop([\"condition\", \"sentenceCondition\", \"RECORDING_SESSION_LABEL\", \"item\"], axis=1)\n",
    "            test_dataset = CustomDataset(test_features, test_labels)\n",
    "            \n",
    "            train_eval_labels = shuffled[\"condition\"].copy()\n",
    "            train_eval_features = shuffled.copy().drop([\"condition\", \"sentenceCondition\", \"RECORDING_SESSION_LABEL\", \"item\"], axis=1)\n",
    "            train_eval_dataset = CustomDataset(train_eval_features, train_eval_labels)\n",
    "\n",
    "\n",
    "            train_eval_split = 0.9\n",
    "            train_size = int(train_eval_split * len(train_eval_dataset))\n",
    "            validation_size = len(train_eval_dataset) - train_size\n",
    "            train_dataset, validation_dataset = torch.utils.data.random_split(train_eval_dataset, [train_size, validation_size])\n",
    "\n",
    "            # class weights for weighted cross-entropy loss (to handle class imbalance)\n",
    "            y = torch.tensor([label for _, label in train_dataset], dtype=torch.long)\n",
    "            sum = y.sum().item()\n",
    "            weight = alpha*(len(train_dataset)-sum) / sum\n",
    "            pos_weights.append(torch.tensor(weight, dtype=torch.float))\n",
    "\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "            test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "            dataloaders.append((train_dataloader, validation_dataloader, test_dataloader))\n",
    "        return dataloaders, pos_weights\n",
    "    \n",
    "    elif task == 3: # Held-out items, known subjects\n",
    "        test_items_count = 0\n",
    "        for i, item in enumerate(data.groups.keys()):\n",
    "            test = data.get_group(item)\n",
    "            test_items_count += len(test)\n",
    "            train_eval = pd.concat([data.get_group(i) for i in data.groups.keys() if i != item])\n",
    "            shuffled = train_eval.sample(frac = 1, random_state=seed) # shuffle the data -> wrecked.\n",
    "            \n",
    "            # splitting data into features and labels for dataset creation\n",
    "            test_labels = test[\"condition\"].copy()\n",
    "            test_features = test.copy().drop([\"condition\", \"sentenceCondition\", \"RECORDING_SESSION_LABEL\", \"item\"], axis=1)\n",
    "            test_dataset = CustomDataset(test_features, test_labels)\n",
    "            \n",
    "            train_eval_labels = shuffled[\"condition\"].copy()\n",
    "            train_eval_features = shuffled.copy().drop([\"condition\", \"sentenceCondition\", \"RECORDING_SESSION_LABEL\", \"item\"], axis=1)\n",
    "            train_eval_dataset = CustomDataset(train_eval_features, train_eval_labels)\n",
    "\n",
    "\n",
    "            train_eval_split = 0.9\n",
    "            train_size = int(train_eval_split * len(train_eval_dataset))\n",
    "            validation_size = len(train_eval_dataset) - train_size\n",
    "            train_dataset, validation_dataset = torch.utils.data.random_split(train_eval_dataset, [train_size, validation_size])\n",
    "\n",
    "            # class weights for weighted cross-entropy loss (to handle class imbalance)\n",
    "            y = torch.tensor([label for _, label in train_dataset], dtype=torch.long)\n",
    "            sum = y.sum().item()\n",
    "            weight = alpha*(len(train_dataset)-sum) / sum\n",
    "            pos_weights.append(torch.tensor(weight, dtype=torch.float))\n",
    "\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "            test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "            dataloaders.append((train_dataloader, validation_dataloader, test_dataloader))\n",
    "        return dataloaders, pos_weights\n",
    "    else:\n",
    "        raise ValueError(\"Task argument must be either 1, 2, or 3\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_split_data(data, batch_size, task, alpha):\n",
    "    \"\"\"\n",
    "    Preprocesses the provided data by normalizing features, mapping \"condition\" to {0,1},\n",
    "    and then splits the data into training, validation, and testing sets based on the task. \n",
    "    This function also prepares DataLoader objects for each split.\n",
    "    The preprocessing steps include selecting critical (non-filler) rows, and normalizing eye-movement features.\n",
    "    After preprocessing, the data is split according to the task.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The raw dataset to preprocess and split. Kindly provided by Iza+Rita.\n",
    "        batch_size (int): The size of each batch to be loaded by the DataLoader.\n",
    "        task (int): An integer indicating the type of task to prepare data for. Valid values are:\n",
    "            0 - Known subjects and items with k-fold cross-validation.\n",
    "            1 - Known subjects and items with leave-one-out cross-validation.\n",
    "            2 - Held-out subjects, known items.\n",
    "            3 - Held-out items, known subjects.\n",
    "        alpha (float): A scaling factor used in computing positive class weights for imbalance handling with BCELoss.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two elements:\n",
    "            - A list of tuples, each containing DataLoader objects for the training, validation, \n",
    "              and testing sets in that order.\n",
    "            - A list of torch.tensor objects representing the alpha-scaled positive class weights computed for \n",
    "              each split to address class imbalance.\n",
    "    \"\"\"\n",
    "\n",
    "# all tasks\n",
    "    data_copy = data.loc[data['is_critical'] == 1].copy()\n",
    "    dropped = data_copy.drop(['composite', 'LF', 'HF', \"IA_ID\", \"trial\", \"list\", \"IA_LABEL\", \"wordlength\", \"is_critical\", \n",
    "                'is_spill1', 'is_spill2', 'is_spill3', 'filler', 'function_word', 'other_filler'], axis=1)\n",
    "\n",
    "    # normalizing input features beforehand, increased performance vs adding batchnorm layer to model\n",
    "    temp = dropped[['fixation_duration',\n",
    "        'duration_firstpass', 'duration_firstfixation', 'fix_count',\n",
    "        'avg_pupil', 'IA_REGRESSION_IN_COUNT', 'IA_REGRESSION_OUT_COUNT',\n",
    "        'saccade_length', 'saccade_duration', 'go_past_time']]\n",
    "    temp = (temp - temp.mean()) / temp.std()\n",
    "    dropped[['fixation_duration',\n",
    "        'duration_firstpass', 'duration_firstfixation', 'fix_count',\n",
    "        'avg_pupil', 'IA_REGRESSION_IN_COUNT', 'IA_REGRESSION_OUT_COUNT',\n",
    "        'saccade_length', 'saccade_duration', 'go_past_time']] = temp\n",
    "    normalized = dropped\n",
    "    # mapping condition and sentenceCondition to 0 and 1 for critical word classification\n",
    "    normalized[[\"condition\", \"sentenceCondition\"]] = normalized[[\"condition\", \"sentenceCondition\"]].map(lambda x: x.replace(\"none\", \"0\"))\n",
    "    normalized[[\"condition\", \"sentenceCondition\"]] = normalized[[\"condition\", \"sentenceCondition\"]].map(lambda x: x.replace(\"control\", \"0\"))\n",
    "    normalized[[\"condition\", \"sentenceCondition\"]] = normalized[[\"condition\", \"sentenceCondition\"]].map(lambda x: x.replace(\"pseudo\", \"1\"))\n",
    "    normalized[[\"condition\", \"sentenceCondition\"]] = normalized[[\"condition\", \"sentenceCondition\"]].map(lambda x: x.replace(\"filler\", \"0\"))\n",
    "    normalized[[\"condition\", \"sentenceCondition\"]] = normalized[[\"condition\", \"sentenceCondition\"]].astype(int)\n",
    "    mapped = normalized\n",
    "\n",
    "# task specific steps\n",
    "    if task == 0 or task == 1: # Known subjects and items\n",
    "        shuffled = mapped.sample(frac = 1, random_state=seed) # shuffle the data -> wrecked.\n",
    "        # splitting data into features and labels for dataset creation\n",
    "        labels = shuffled[\"condition\"].copy()\n",
    "        features = shuffled.copy().drop([\"condition\", \"sentenceCondition\", \"RECORDING_SESSION_LABEL\", \"item\"], axis=1)\n",
    "        data = (features, labels)\n",
    "        return split_data(data, batch_size, task, alpha)\n",
    "    \n",
    "    elif task == 2: # Held-out subjects, known items\n",
    "        subjects = mapped.groupby('RECORDING_SESSION_LABEL')\n",
    "        return split_data(subjects, batch_size, task, alpha)\n",
    "    elif task == 3: # Held-out items, known subjects\n",
    "        items = mapped.groupby('item')\n",
    "        return split_data(items, batch_size, task, alpha)\n",
    "    else:\n",
    "        raise ValueError(\"Task argument must be either 1, 2, or 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(model, dataloader, optimizer, pos_weight, training):\n",
    "    \"\"\"\n",
    "    Performs a single epoch of training, validation, or testing on the given model using the specified DataLoader. \n",
    "    This function adapts its behavior based on the 'training' parameter to correctly handle the model's state and \n",
    "    perform necessary operations such as backpropagation and optimizer updates during training.\n",
    "\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): The neural network model to be trained, validated, or tested.\n",
    "        dataloader (DataLoader): A DataLoader providing batches of data (features and labels) for processing.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer (AdamW) to use for updating model parameters during training.\n",
    "        pos_weight (torch.Tensor): A tensor specifying the weight for the positive class to handle class imbalance.\n",
    "        training (str): A string specifying the mode of operation. Must be 'train', 'validation', or 'test'.\n",
    "\n",
    "    Returns:\n",
    "        None if training.\n",
    "        Cumulative loss (float) if validation.\n",
    "        A tuple (label_list, prediction_list) containing lists of true labels and predicted labels for \n",
    "        each sample if testing.\n",
    "    \"\"\"\n",
    "    # BCEWithLogitsLoss combines sigmoid with BCELoss for better stability, and handles class imbalance via pos_weight\n",
    "    loss_function = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    if training == \"train\":\n",
    "        model.train()\n",
    "    elif training == \"validation\":\n",
    "        model.eval()\n",
    "    elif training == \"test\":\n",
    "        model.eval()\n",
    "    else:\n",
    "        raise ValueError(\"training argument must be either 'train', 'validation' or 'test'\")\n",
    "        \n",
    "    cumulative_loss = 0\n",
    "    prediction_list = [] # store predictions accross folds for calculating accuracy and f1\n",
    "    label_list = [] # store labels accross folds for calculating accuracy and f1\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    for sample in dataloader: # iterate over batches in the DataLoader\n",
    "        data, targets = sample[0].float().to(device), sample[1].type(torch.LongTensor).to(device)\n",
    "        output = model(data) # forward pass\n",
    "        loss_value = loss_function(output, targets.unsqueeze(1).float())\n",
    "        cumulative_loss += loss_value.item()\n",
    "\n",
    "        if training == \"train\":\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        predictions = [round(x) for x in sigmoid(output).to('cpu').detach().squeeze(1).numpy().tolist()] # gets {0,1} predictions from 1d logits\n",
    "        target_labels = targets.to('cpu').detach().numpy()\n",
    "        prediction_list.extend(predictions)\n",
    "        label_list.extend(target_labels)\n",
    "\n",
    "    if training == \"train\":\n",
    "        return\n",
    "    elif training == \"validation\":\n",
    "        return cumulative_loss\n",
    "    elif training == \"test\":\n",
    "        return label_list, prediction_list\n",
    "    else:\n",
    "        raise ValueError(\"Ya Done Fuck'd up, son!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TuneableModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A customizable neural network model for binary classification tasks, with a variable number of layers \n",
    "    and dropout rate. The model architecture consists of an input layer followed by a configurable number \n",
    "    of hidden layers, and an output layer. Each hidden layer includes a linear transformation, an activation \n",
    "    function (LeakyReLU), and (optional) dropout for regularization. Batch normalization can be added as needed, \n",
    "    but ended up unused in the final version because normalizing the dataset before training resulted in better predictions.\n",
    "\n",
    "    Parameters:\n",
    "        input_size (int): The number of input features.\n",
    "        layer_size (int): The size of each hidden layer.\n",
    "        dropout_rate (float): The dropout rate for regularization, applied to each hidden layer.\n",
    "        n_layers (int): The number of hidden layers in the network, ranging from 1 to 10.\n",
    "\n",
    "    Attributes:\n",
    "        input_layer (Linear): The input layer.\n",
    "        linear2 to linear10 (Linear): Optional hidden layers, activated based on `n_layers`.\n",
    "        output_layer (Linear): The output layer for binary classification output.\n",
    "        dropout (Dropout): Dropout layer for regularization.\n",
    "        activation (LeakyReLU): Activation function used after each linear layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, layer_size, dropout_rate, n_layers):\n",
    "        super(TuneableModel, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.input_size = input_size\n",
    "        self.input_layer = torch.nn.Linear(input_size, layer_size)\n",
    "        # is there a way to do this not manually? :-\\\n",
    "        self.linear2 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear3 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear4 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear5 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear6 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear7 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear8 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear9 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear10 = torch.nn.Linear(layer_size, layer_size)\n",
    "\n",
    "        self.output_layer = torch.nn.Linear(layer_size, 1)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate) # rarely useful...\n",
    "        self.activation = torch.nn.LeakyReLU() # oh, I forgot I'd chosen LeakyReLU!! uh oh...\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model. Applies linear transformations, activation functions, and dropout \n",
    "        sequentially based on the configured number of layers (`n_layers`), and finally outputs the prediction.\n",
    "\n",
    "        Parameters:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, input_size).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output predictions of shape (batch_size, 1).\n",
    "        \"\"\"\n",
    "        x = self.input_layer(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        if self.n_layers > 1:\n",
    "            x = self.linear2(x)\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout(x)\n",
    "            if self.n_layers > 2:\n",
    "                x = self.linear3(x)\n",
    "                x = self.activation(x)\n",
    "                x = self.dropout(x)\n",
    "                if self.n_layers > 3:\n",
    "                    x = self.linear4(x)\n",
    "                    x = self.activation(x)\n",
    "                    x = self.dropout(x)\n",
    "                    if self.n_layers > 4:\n",
    "                        x = self.linear5(x)\n",
    "                        x = self.activation(x)\n",
    "                        x = self.dropout(x)\n",
    "                        if self.n_layers > 5:\n",
    "                            x = self.linear6(x)\n",
    "                            x = self.activation(x)\n",
    "                            x = self.dropout(x)\n",
    "                            if self.n_layers > 6:\n",
    "                                x = self.linear7(x)\n",
    "                                x = self.activation(x)\n",
    "                                x = self.dropout(x)\n",
    "                                if self.n_layers > 7:\n",
    "                                    x = self.linear8(x)\n",
    "                                    x = self.activation(x)\n",
    "                                    x = self.dropout(x)\n",
    "                                    if self.n_layers > 8:\n",
    "                                        x = self.linear9(x)\n",
    "                                        x = self.activation(x)\n",
    "                                        x = self.dropout(x)\n",
    "                                        if self.n_layers > 9:\n",
    "                                            x = self.linear10(x)\n",
    "                                            x = self.activation(x)\n",
    "                                            x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training sample\n",
    "def evaluate(data, parameters, task):\n",
    "    \"\"\"\n",
    "    Evaluates neural model's performance on a given task using specified parameters. \n",
    "    The function preprocesses the data, splits it according to the task, initializes a TuneableModel, \n",
    "    and trains it. It then evaluates the model on a test set and returns performance metrics.\n",
    "\n",
    "    The function asserts the task to be one of the predefined tasks and initializes the model based on \n",
    "    the provided parameters. It supports dynamic pos_weight handling and uses early stopping based on \n",
    "    validation loss to prevent overfitting.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The dataset to evaluate the model on.\n",
    "        parameters (dict): A dictionary containing model hyperparameters and training settings. Expected \n",
    "            keys include \"pos_weight\", \"batch_size\", \"alpha\", \"hidden_size\", \"dropout\", \"n_hidden\", \n",
    "            \"learning_rate\", \"beta_1\", and \"beta_2\".\n",
    "        task (int): An integer indicating the task type. Valid values are 0, 1, 2, and 3, each representing \n",
    "            a different way of splitting the data for training and testing:\n",
    "                0 - Known subjects and items with k-fold cross-validation.\n",
    "                1 - Known subjects and items with leave-one-out cross-validation.\n",
    "                2 - Held-out subjects, known items.\n",
    "                3 - Held-out items, known subjects.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the accuracy score, F1 score, and confusion matrix of the model evaluated \n",
    "            on a given test set.\n",
    "    \"\"\"\n",
    "    assert task in [0, 1, 2, 3], \"Task argument must be either 1, 2 or 3\"\n",
    "    \n",
    "    weight = None if parameters[\"pos_weight\"] == None else torch.tensor(parameters[\"pos_weight\"], dtype=torch.float).to(device)\n",
    "\n",
    "    max_epochs = 1000\n",
    "\n",
    "    dataloaders, class_weights = preprocess_and_split_data(data, parameters[\"batch_size\"], task, alpha=parameters[\"alpha\"])\n",
    "\n",
    "    input_size = 10 # number of features :( -> this is hardcoded for now, try to get it from the dataset\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    torch.manual_seed(seed)\n",
    "    model = TuneableModel(input_size, parameters[\"hidden_size\"], parameters[\"dropout\"], parameters[\"n_hidden\"])\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=parameters[\"learning_rate\"], \n",
    "                                  betas=(parameters[\"beta_1\"], parameters[\"beta_2\"]), weight_decay=1e-2)\n",
    "\n",
    "    for i, dataloader in tqdm(enumerate(dataloaders)):\n",
    "        pos_weight = class_weights[i] if weight is None else weight\n",
    "        max_patience = 10 if i < 35 else 2\n",
    "        last_loss = 1000000\n",
    "        PATH = f\"./models/model_{i}.pt\"\n",
    "        train_dataloader, validation_dataloader, test_dataloader = dataloader[0], dataloader[1], dataloader[2]\n",
    "        for epoch in range(max_epochs):\n",
    "            # training\n",
    "            train_test(model, train_dataloader, optimizer, pos_weight, training=\"train\")\n",
    "            # validation at end of epoch\n",
    "            with torch.no_grad():\n",
    "                validation_loss = train_test(model, validation_dataloader, optimizer, pos_weight, training=\"validation\")\n",
    "    \n",
    "            if validation_loss < last_loss:\n",
    "                last_loss = validation_loss\n",
    "                current_patience = 0\n",
    "            else:\n",
    "                if current_patience == 0:\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': last_loss,\n",
    "                        }, PATH)\n",
    "                current_patience += 1\n",
    "            if current_patience == max_patience:\n",
    "                break   \n",
    "\n",
    "        # Testing once patience is reached\n",
    "        torch.manual_seed(seed)\n",
    "        model = TuneableModel(input_size, parameters[\"hidden_size\"], parameters[\"dropout\"], parameters[\"n_hidden\"])\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=parameters[\"learning_rate\"], betas=(0.99, 0.99), weight_decay=1e-4)\n",
    "        checkpoint = torch.load(PATH)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        with torch.no_grad():\n",
    "            prediction_list, label_list = train_test(model, test_dataloader, optimizer, pos_weight, training=\"test\")\n",
    "        predictions.extend(prediction_list)\n",
    "        labels.extend(label_list)\n",
    "\n",
    "    return accuracy_score(labels, predictions), f1_score(labels, predictions), confusion_matrix(labels, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Parameters:\n",
    "    Patience = 10\n",
    "    @ model_params(\n",
    "        dropout: 0.0\n",
    "        layer size: 500\n",
    "        lr: 0.001\n",
    "        batch_size: 16\n",
    "        n_layers: 6)\n",
    "    @ optimizer_AdamW(\n",
    "        beta_1: 0.999,\n",
    "        beta_2: 0.999, \n",
    "        weight_decay: 1e-2)\n",
    "    @ BCEWithLogitsLoss(\n",
    "        pos_weight: vairable)\n",
    "\n",
    "### Performance:\n",
    "\n",
    "    Task 1: Known subjects, known items\n",
    "\n",
    "        Train:Eval:Test - 80:10:10 - 10-fold\n",
    "            @ alpha = 0.21\n",
    "        Acc: 98.25%\n",
    "        F1: 0.9643\n",
    "        Confusion:\n",
    "            [[256   4]\n",
    "            [  2  81]]\n",
    "\n",
    "        Blind LOOCV: Train:Eval:Test - n-2:1:1\n",
    "            @ pos_weight = 0.65\n",
    "        Acc: 100.0%\n",
    "        F1: 1.0\n",
    "        Confusion:\n",
    "            [[258   0]\n",
    "            [  0  85]]\n",
    "            \n",
    "    Task 2: Left-out subjects, known items\n",
    "    \n",
    "        Train:eval:test - 90:10:(1 subject) <- sadface\n",
    "            @ alpha = 0.21\n",
    "        Acc: 98.54%\n",
    "        F1: 0.9701\n",
    "        Confusion:\n",
    "            [[257   4]\n",
    "            [  1  81]]\n",
    "        \n",
    "        Train:eval:test - 90:10:(2 subjects)\n",
    "            @ alpha = 0.21\n",
    "        Acc: 97.08%\n",
    "        F1: 0.9405\n",
    "        Confusion:\n",
    "            [[254   6]\n",
    "            [  4  79]]\n",
    "\n",
    "    Task 3: Left-out items, known subjects, train:eval:test - 90:10:item\n",
    "            @ pos_weight = 0.8, dropout =0.01\n",
    "        Acc: 97.08%\n",
    "        F1: 0.9419\n",
    "        Confusion:\n",
    "            [[252   4]\n",
    "            [  6  81]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: Known subjects and items, 10-fold CV\n",
      "Training on cuda with parameters: \n",
      "@ model_params(\n",
      "\tdropout=0.0,      \n",
      "\thidden_size=500,\n",
      "\tlearning_rate=0.001,      \n",
      "\tbatch_size=16,\n",
      "\tn_hidden=6),\n",
      "@ optimizer_AdamW(\n",
      "\tbeta_1=0.999,\n",
      "\tbeta_2=0.999)\n",
      "@ BCEWithLogitsLoss(\n",
      "\talpha=0.21)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:52,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 96.5%\n",
      "F1: 0.9259\n",
      "Confusion:\n",
      " [[256  10]\n",
      " [  2  75]] \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# set up the directory for saving models\n",
    "directory = \"./models/\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "    \n",
    "parameters = {\n",
    "    \"dropout\": 0.0,\n",
    "    \"hidden_size\": 500,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 16,\n",
    "    \"n_hidden\": 6,\n",
    "    \"beta_1\": 0.999,\n",
    "    \"beta_2\": 0.999,\n",
    "    \"alpha\": 0.21\n",
    "}\n",
    "\n",
    "\n",
    "tasks = [\"Known subjects and items, 10-fold CV\", \"Known subjects and items, LOOCV\", \"2 Held-out subjects, known items\", \"1 Held-out items, known subjects\"]\n",
    "\n",
    "for task in [0, 1, 2, 3]:\n",
    "    if task == 0:\n",
    "        parameters[\"alpha\"] = 0.21\n",
    "        parameters[\"pos_weight\"] = 0.7\n",
    "    elif task == 1:\n",
    "        parameters[\"alpha\"] = 1\n",
    "        parameters[\"pos_weight\"] = 0.65\n",
    "    elif task == 2:\n",
    "        parameters[\"alpha\"] = 0.19\n",
    "        parameters[\"pos_weight\"] = None\n",
    "    elif task == 3:\n",
    "        parameters[\"alpha\"] = 1\n",
    "        parameters[\"pos_weight\"] = .8\n",
    "        parameters[\"dropout\"] = 0.01 # dumb.\n",
    "        parameters[\"beta_2\"] = 0.99\n",
    "\n",
    "    print(f\"Task: {tasks[task]}\")\n",
    "    print(f'Training on {device} with parameters: \\n@ model_params(\\n\\tdropout={parameters[\"dropout\"]},\\\n",
    "      \\n\\thidden_size={parameters[\"hidden_size\"]},\\n\\tlearning_rate={parameters[\"learning_rate\"]},\\\n",
    "      \\n\\tbatch_size={parameters[\"batch_size\"]},\\n\\tn_hidden={parameters[\"n_hidden\"]}),')\n",
    "    print(f'@ optimizer_AdamW(\\n\\tbeta_1={parameters[\"beta_1\"]},\\n\\tbeta_2={parameters[\"beta_2\"]})')\n",
    "    print(f'@ BCEWithLogitsLoss(\\n\\talpha={parameters[\"alpha\"]})\\n')\n",
    "    accuracy, f1, confusion = evaluate(data, parameters, task)\n",
    "    print(f\"Acc: {round(accuracy*100,2)}%\\nF1: {round(f1,4)}\")\n",
    "    print(\"Confusion:\\n\", confusion, \"\\n\\n\")\n",
    "\n",
    "    # remove all files in the models directory after training and evaluation\n",
    "    shutil.rmtree(directory, ignore_errors=True)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
