{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hi :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RECORDING_SESSION_LABEL</th>\n",
       "      <th>trial</th>\n",
       "      <th>IA_ID</th>\n",
       "      <th>item</th>\n",
       "      <th>list</th>\n",
       "      <th>IA_LABEL</th>\n",
       "      <th>wordlength</th>\n",
       "      <th>condition</th>\n",
       "      <th>is_critical</th>\n",
       "      <th>is_spill1</th>\n",
       "      <th>...</th>\n",
       "      <th>duration_firstpass</th>\n",
       "      <th>duration_firstfixation</th>\n",
       "      <th>fix_count</th>\n",
       "      <th>avg_pupil</th>\n",
       "      <th>IA_REGRESSION_IN_COUNT</th>\n",
       "      <th>IA_REGRESSION_OUT_COUNT</th>\n",
       "      <th>saccade_length</th>\n",
       "      <th>saccade_duration</th>\n",
       "      <th>go_past_time</th>\n",
       "      <th>sentenceCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10m23r2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Viel</td>\n",
       "      <td>4</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>236</td>\n",
       "      <td>236</td>\n",
       "      <td>1</td>\n",
       "      <td>1408.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72.376792</td>\n",
       "      <td>16</td>\n",
       "      <td>236</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10m23r2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Geld</td>\n",
       "      <td>4</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>424</td>\n",
       "      <td>264</td>\n",
       "      <td>3</td>\n",
       "      <td>1379.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>71.519648</td>\n",
       "      <td>160</td>\n",
       "      <td>424</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10m23r2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>wurde</td>\n",
       "      <td>5</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10m23r2</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>investiert,</td>\n",
       "      <td>11</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>420</td>\n",
       "      <td>268</td>\n",
       "      <td>3</td>\n",
       "      <td>1290.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>65.401223</td>\n",
       "      <td>12</td>\n",
       "      <td>420</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10m23r2</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>bevor</td>\n",
       "      <td>5</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>296</td>\n",
       "      <td>296</td>\n",
       "      <td>2</td>\n",
       "      <td>1242.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>42.311819</td>\n",
       "      <td>12</td>\n",
       "      <td>296</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11171</th>\n",
       "      <td>9m23r1</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>Verteidigung</td>\n",
       "      <td>12</td>\n",
       "      <td>filler</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>168</td>\n",
       "      <td>168</td>\n",
       "      <td>2</td>\n",
       "      <td>497.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>91.810675</td>\n",
       "      <td>20</td>\n",
       "      <td>168</td>\n",
       "      <td>filler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11172</th>\n",
       "      <td>9m23r1</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>Europas</td>\n",
       "      <td>7</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>1</td>\n",
       "      <td>493.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>119.788355</td>\n",
       "      <td>24</td>\n",
       "      <td>224</td>\n",
       "      <td>filler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11173</th>\n",
       "      <td>9m23r1</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>erhöht</td>\n",
       "      <td>6</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "      <td>1</td>\n",
       "      <td>472.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>95.117033</td>\n",
       "      <td>20</td>\n",
       "      <td>332</td>\n",
       "      <td>filler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11174</th>\n",
       "      <td>9m23r1</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>werden</td>\n",
       "      <td>6</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>244</td>\n",
       "      <td>244</td>\n",
       "      <td>1</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.649126</td>\n",
       "      <td>16</td>\n",
       "      <td>244</td>\n",
       "      <td>filler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11175</th>\n",
       "      <td>9m23r1</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>sollten.</td>\n",
       "      <td>8</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>495.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1164</td>\n",
       "      <td>filler</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11176 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      RECORDING_SESSION_LABEL  trial  IA_ID  item  list      IA_LABEL  \\\n",
       "0                     10m23r2     12      1     1     2          Viel   \n",
       "1                     10m23r2     12      2     1     2          Geld   \n",
       "2                     10m23r2     12      3     1     2         wurde   \n",
       "3                     10m23r2     12      4     1     2   investiert,   \n",
       "4                     10m23r2     12      5     1     2         bevor   \n",
       "...                       ...    ...    ...   ...   ...           ...   \n",
       "11171                  9m23r1     10     12    28     1  Verteidigung   \n",
       "11172                  9m23r1     10     13    28     1       Europas   \n",
       "11173                  9m23r1     10     14    28     1        erhöht   \n",
       "11174                  9m23r1     10     15    28     1        werden   \n",
       "11175                  9m23r1     10     16    28     1      sollten.   \n",
       "\n",
       "       wordlength condition  is_critical  is_spill1  ...  duration_firstpass  \\\n",
       "0               4      none            0          0  ...                 236   \n",
       "1               4      none            0          0  ...                 424   \n",
       "2               5      none            0          0  ...                   0   \n",
       "3              11      none            0          0  ...                 420   \n",
       "4               5      none            0          0  ...                 296   \n",
       "...           ...       ...          ...        ...  ...                 ...   \n",
       "11171          12    filler            0          0  ...                 168   \n",
       "11172           7      none            0          0  ...                 224   \n",
       "11173           6      none            0          0  ...                 332   \n",
       "11174           6      none            0          0  ...                 244   \n",
       "11175           8      none            0          0  ...                 160   \n",
       "\n",
       "       duration_firstfixation  fix_count    avg_pupil  IA_REGRESSION_IN_COUNT  \\\n",
       "0                         236          1  1408.000000                       0   \n",
       "1                         264          3  1379.333333                       2   \n",
       "2                           0          0     0.000000                       0   \n",
       "3                         268          3  1290.000000                       1   \n",
       "4                         296          2  1242.500000                       1   \n",
       "...                       ...        ...          ...                     ...   \n",
       "11171                     168          2   497.000000                       0   \n",
       "11172                     224          1   493.000000                       0   \n",
       "11173                     332          1   472.000000                       0   \n",
       "11174                     244          1   477.000000                       0   \n",
       "11175                     160          1   495.000000                       0   \n",
       "\n",
       "       IA_REGRESSION_OUT_COUNT  saccade_length  saccade_duration  \\\n",
       "0                            0       72.376792                16   \n",
       "1                            0       71.519648               160   \n",
       "2                            1        0.000000                 0   \n",
       "3                            0       65.401223                12   \n",
       "4                            0       42.311819                12   \n",
       "...                        ...             ...               ...   \n",
       "11171                        0       91.810675                20   \n",
       "11172                        0      119.788355                24   \n",
       "11173                        0       95.117033                20   \n",
       "11174                        0       58.649126                16   \n",
       "11175                        1        0.000000                 0   \n",
       "\n",
       "       go_past_time  sentenceCondition  \n",
       "0               236            control  \n",
       "1               424            control  \n",
       "2                 0            control  \n",
       "3               420            control  \n",
       "4               296            control  \n",
       "...             ...                ...  \n",
       "11171           168             filler  \n",
       "11172           224             filler  \n",
       "11173           332             filler  \n",
       "11174           244             filler  \n",
       "11175          1164             filler  \n",
       "\n",
       "[11176 rows x 29 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import product # used for hyperparameter grid search, unused if not doing hyperparameter tuning\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seed = 42 # for reproducibility\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.features.iloc[index].to_numpy()\n",
    "        label = self.labels.iloc[index]\n",
    "        return features, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset, batch_size, task=1):\n",
    "\n",
    "    k=10\n",
    "    dataloaders = []\n",
    "\n",
    "\n",
    "    if task == 1: # Known subjects and items\n",
    "        n = len(dataset)\n",
    "        fold_size = n // k\n",
    "        folds = []\n",
    "        for i in range(k):\n",
    "            start = i * fold_size\n",
    "            end = (i + 1) * fold_size if i < k - 1 else n\n",
    "            folds.append(torch.utils.data.Subset(dataset, range(start, end)))\n",
    "\n",
    "        for i in range(k):\n",
    "            # splits for cross-validation, validation set = test set (since we're doing k-fold, we won't use a separate test set)\n",
    "            validation_dataset = folds[i]\n",
    "            #t = i + 1 if i < k - 1 else 0\n",
    "            #test_dataset = folds[t]\n",
    "            train_folds = [folds[j] for j in range(k) if j != i]# and j != t]\n",
    "            train_dataset = torch.utils.data.ConcatDataset(train_folds)\n",
    "\n",
    "            # class weights for weighted cross-entropy loss (to handle class imbalance)\n",
    "            y = torch.tensor([label for _, label in train_dataset], dtype=torch.long)\n",
    "            global class_weights\n",
    "            class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y.numpy())\n",
    "            class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "            #test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "            #dataloaders.append((train_dataloader, validation_dataloader, test_dataloader))\n",
    "            dataloaders.append((train_dataloader, validation_dataloader))\n",
    "\n",
    "        return dataloaders\n",
    "    \n",
    "    elif task == 2: # Held-out subjects, known items # TODO\n",
    "        raise NotImplementedError(\"Task 2 not implemented yet\")\n",
    "    elif task == 3: # Held-out items, known subjects # TODO\n",
    "        raise NotImplementedError(\"Task 3 not implemented yet\")\n",
    "    else:\n",
    "        raise ValueError(\"Task argument must be either 1, 2, or 3\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_split_data(data, batch_size=32, task=1):\n",
    "    \n",
    "# all tasks\n",
    "    \n",
    "    data_copy = data.loc[data['is_critical'] == 1].copy()\n",
    "    dropped = data_copy.drop(['composite', 'LF', 'HF', \"RECORDING_SESSION_LABEL\", \"trial\", \"IA_ID\", \"item\", \"list\", \"IA_LABEL\", \"wordlength\", \"is_critical\", \n",
    "                'is_spill1', 'is_spill2', 'is_spill3', 'filler', 'function_word', 'other_filler'], axis=1)\n",
    "    print(\"Original dataset size: \", len(data_copy))\n",
    "\n",
    "    # normalizing input features beforehand, increased performance vs adding batchnorm layer to model\n",
    "    temp = dropped[['fixation_duration',\n",
    "        'duration_firstpass', 'duration_firstfixation', 'fix_count',\n",
    "        'avg_pupil', 'IA_REGRESSION_IN_COUNT', 'IA_REGRESSION_OUT_COUNT',\n",
    "        'saccade_length', 'saccade_duration', 'go_past_time']]\n",
    "    temp = (temp - temp.mean()) / temp.std()\n",
    "    dropped[['fixation_duration',\n",
    "        'duration_firstpass', 'duration_firstfixation', 'fix_count',\n",
    "        'avg_pupil', 'IA_REGRESSION_IN_COUNT', 'IA_REGRESSION_OUT_COUNT',\n",
    "        'saccade_length', 'saccade_duration', 'go_past_time']] = temp\n",
    "    normalized = dropped\n",
    "# task 1 specific steps\n",
    "    if task == 1: # Known subjects and items\n",
    "        # mapping condition and sentenceCondition to 0 and 1 for critical word classification\n",
    "        normalized[[\"condition\", \"sentenceCondition\"]] = normalized[[\"condition\", \"sentenceCondition\"]].map(lambda x: x.replace(\"none\", \"0\"))\n",
    "        normalized[[\"condition\", \"sentenceCondition\"]] = normalized[[\"condition\", \"sentenceCondition\"]].map(lambda x: x.replace(\"control\", \"0\"))\n",
    "        normalized[[\"condition\", \"sentenceCondition\"]] = normalized[[\"condition\", \"sentenceCondition\"]].map(lambda x: x.replace(\"pseudo\", \"1\"))\n",
    "        normalized[[\"condition\", \"sentenceCondition\"]] = normalized[[\"condition\", \"sentenceCondition\"]].map(lambda x: x.replace(\"filler\", \"0\"))\n",
    "        normalized[[\"condition\", \"sentenceCondition\"]] = normalized[[\"condition\", \"sentenceCondition\"]].astype(int)\n",
    "        mapped = normalized\n",
    "\n",
    "        shuffled = mapped.sample(frac = 1, random_state=seed) # shuffle the data -> wrecked.\n",
    "        # splitting data into features and labels for dataset creation\n",
    "        labels = shuffled[\"condition\"].copy()\n",
    "        features = shuffled.copy().drop([\"condition\", \"sentenceCondition\"], axis=1)\n",
    "\n",
    "        print(\"Preprocessed dataset size: \", len(features))\n",
    "        return split_data(CustomDataset(features=features, labels=labels), batch_size, task)\n",
    "    \n",
    "    elif task == 2: # Held-out subjects, known items # TODO\n",
    "        raise NotImplementedError(\"Task 2 not implemented yet\")\n",
    "        return split_data(CustomDataset(features=features, labels=labels), batch_size, task)\n",
    "    elif task == 3: # Held-out items, known subjects # TODO\n",
    "        raise NotImplementedError(\"Task 3 not implemented yet\")\n",
    "        return split_data(CustomDataset(features=features, labels=labels), batch_size, task)\n",
    "    else:\n",
    "        raise ValueError(\"Task argument must be either 1, 2, or 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(model, dataloader, optimizer, training=\"train\"):\n",
    "   \n",
    "    loss_function = torch.nn.BCEWithLogitsLoss()#weight=class_weights.to(device))\n",
    "\n",
    "    if training == \"train\":\n",
    "        model.train()\n",
    "    elif training == \"validation\":\n",
    "        model.eval()\n",
    "    elif training == \"test\":\n",
    "        model.eval()\n",
    "    else:\n",
    "        raise ValueError(\"training argument must be either 'train', 'validation' or 'test'\")\n",
    "        \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    cumulative_loss = 0\n",
    "    prediction_list = []\n",
    "    label_list = []\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    for sample in dataloader:\n",
    "   \n",
    "        data, targets = sample[0].float().to(device), sample[1].type(torch.LongTensor).to(device)\n",
    "        output = model(data)\n",
    "        loss_value = loss_function(output, targets.unsqueeze(1).float())\n",
    "        cumulative_loss += loss_value.item()\n",
    "\n",
    "        if training == \"train\":\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        predictions = [round(x) for x in sigmoid(output).to('cpu').detach().squeeze(1).numpy().tolist()]#.argmax(axis=1)\n",
    "        target_labels = targets.to('cpu').detach().numpy()\n",
    "        total += len(predictions)\n",
    "        correct += accuracy_score(target_labels, predictions, normalize=False)\n",
    "        prediction_list.extend(predictions)\n",
    "        label_list.extend(target_labels)\n",
    "    if training == \"test\":\n",
    "        return label_list, prediction_list\n",
    "    f1 = f1_score(label_list, prediction_list)\n",
    "    accuracy = accuracy_score(label_list, prediction_list)\n",
    "    confusion = confusion_matrix(label_list, prediction_list)\n",
    "\n",
    "    return cumulative_loss, accuracy, f1, confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TuneableModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, layer_size, dropout_rate, n_layers):\n",
    "        super(TuneableModel, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.input_layer = torch.nn.Linear(input_size, layer_size)\n",
    "        self.linear2 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear3 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear4 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear5 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear6 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear7 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear8 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear9 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear10 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.output_layer = torch.nn.Linear(layer_size, 1)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.activation = torch.nn.LeakyReLU()\n",
    "        self.batchnorm = torch.nn.BatchNorm1d(layer_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        #x = self.batchnorm(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        if self.n_layers > 1:\n",
    "            x = self.linear2(x)\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout(x)\n",
    "            if self.n_layers > 2:\n",
    "                x = self.linear3(x)\n",
    "                x = self.activation(x)\n",
    "                x = self.dropout(x)\n",
    "                if self.n_layers > 3:\n",
    "                    x = self.linear4(x)\n",
    "                    x = self.activation(x)\n",
    "                    x = self.dropout(x)\n",
    "                    if self.n_layers > 4:\n",
    "                        x = self.linear5(x)\n",
    "                        x = self.activation(x)\n",
    "                        x = self.dropout(x)\n",
    "                        if self.n_layers > 5:\n",
    "                            x = self.linear6(x)\n",
    "                            x = self.activation(x)\n",
    "                            x = self.dropout(x)\n",
    "                            if self.n_layers > 6:\n",
    "                                x = self.linear7(x)\n",
    "                                x = self.activation(x)\n",
    "                                x = self.dropout(x)\n",
    "                                if self.n_layers > 7:\n",
    "                                    x = self.linear8(x)\n",
    "                                    x = self.activation(x)\n",
    "                                    x = self.dropout(x)\n",
    "                                    if self.n_layers > 8:\n",
    "                                        x = self.linear9(x)\n",
    "                                        x = self.activation(x)\n",
    "                                        x = self.dropout(x)\n",
    "                                        if self.n_layers > 9:\n",
    "                                            x = self.linear10(x)\n",
    "                                            x = self.activation(x)\n",
    "                                            x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        #x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training sample\n",
    "def evaluate(data, parameters, task=1):\n",
    "    assert task in [1, 2, 3], \"Task argument must be either 1, 2 or 3\"\n",
    "    assert task not in [2, 3], \"Task 2 and 3 are not yet implemented\"\n",
    "    \n",
    "    dropout, hidden_size, learning_rate, batch_size, n_hidden = parameters\n",
    "\n",
    "    max_epochs = 1000\n",
    "    max_patience = 10\n",
    "\n",
    "    dataloaders = preprocess_and_split_data(data, batch_size, task=task)\n",
    "\n",
    "    input_size = 10 # number of features :( -> this is hardcoded for now, try to get it from the dataset\n",
    "    best_epochs = []\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    torch.manual_seed(seed)\n",
    "    model = TuneableModel(input_size, hidden_size, dropout, n_hidden)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.99, 0.99), weight_decay=1e-2)\n",
    "\n",
    "\n",
    "    \n",
    "    for i, dataloader in tqdm(enumerate(dataloaders)):\n",
    "        last_loss = 1000000\n",
    "        best_epoch = 0\n",
    "        PATH = f\"model_{i}.pt\"\n",
    "        train_dataloader, validation_dataloader, test_dataloader = dataloader[0], dataloader[1], dataloader[1]\n",
    "        for epoch in range(max_epochs):\n",
    "            # training\n",
    "            train_loss, train_accuracy, train_f1, train_confusion = train_test(model, train_dataloader, optimizer, training=\"train\")\n",
    "            train_loss, train_accuracy, train_f1 = round(train_loss, 2), round(train_accuracy, 4), round(train_f1, 2)\n",
    "            # validation at end of epoch\n",
    "            validation_loss, validation_accuracy, validation_f1, validation_confusion = train_test(model, validation_dataloader, optimizer, training=\"validation\")\n",
    "            validation_loss, validation_accuracy, validation_f1 = round(validation_loss, 2), round(validation_accuracy, 4), round(validation_f1, 2)\n",
    "            if validation_loss < last_loss:\n",
    "                last_loss = validation_loss\n",
    "                best_epoch = epoch\n",
    "                current_patience = 0\n",
    "            else:\n",
    "                if current_patience == 0:\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': last_loss,\n",
    "                        }, PATH)\n",
    "                current_patience += 1\n",
    "            if current_patience == max_patience:\n",
    "                break   \n",
    "            # if epoch % 100 == 0 and epoch != 0:\n",
    "            #     print(f\"Epoch {epoch}: Train loss: {train_loss}, Train accuracy: {train_accuracy}, Train f1: {train_f1}\")\n",
    "            #     print(f\"Epoch {epoch}: Validation loss: {validation_loss}, Validation accuracy: {validation_accuracy}, Validation f1: {validation_f1}\")\n",
    "\n",
    "        # Testing once patience is reached\n",
    "        torch.manual_seed(seed)\n",
    "        model = TuneableModel(input_size, hidden_size, dropout, n_hidden)\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.99, 0.99), weight_decay=1e-4)\n",
    "        checkpoint = torch.load(PATH)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        prediction_list, label_list = train_test(model, test_dataloader, optimizer, training=\"test\")\n",
    "        predictions.extend(prediction_list)\n",
    "        labels.extend(label_list)\n",
    "        best_epochs.append(best_epoch)\n",
    "    print(\"Average training epochs for best model:\", round(np.mean(best_epochs), 1))\n",
    "    print(\"Best epochs:\\n\", best_epochs)\n",
    "    return accuracy_score(labels, predictions), f1_score(labels, predictions), confusion_matrix(labels, predictions)\n",
    "    # print(f\"Average accuracy: {round(np.mean(accuracies), 2)}%\")\n",
    "    # print(f\"Average f1: {round(np.mean(f1s), 2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size:  343\n",
      "Preprocessed dataset size:  343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:12,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training epochs for best model: 1.5\n",
      "Best epochs:\n",
      " [0, 2, 3, 0, 2, 2, 6, 0, 0, 0]\n",
      "Acc: 95.92%\n",
      " f1: 0.92\n",
      "[[249   5]\n",
      " [  9  80]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Task 2 and 3 are not yet implemented",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[117], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m parameters \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m):\n\u001b[1;32m---> 32\u001b[0m     accuracy, f1, confusion \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAcc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(accuracy\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m f1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(f1,\u001b[38;5;241m3\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(confusion)\n",
      "Cell \u001b[1;32mIn[109], line 4\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(data, parameters, task)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(data, parameters, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask argument must be either 1, 2 or 3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m task \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask 2 and 3 are not yet implemented\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m     dropout, hidden_size, learning_rate, batch_size, n_hidden \u001b[38;5;241m=\u001b[39m parameters\n\u001b[0;32m      8\u001b[0m     max_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Task 2 and 3 are not yet implemented"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Performance:\n",
    "    Task 1:\n",
    "        Best parameters:\n",
    "        Patience = 10\n",
    "        @ model_params(\n",
    "            dropout: 0.0\n",
    "            layer size: 500\n",
    "            lr: 0.001\n",
    "            batch_size: 32\n",
    "            n_layers: 5)\n",
    "        @ optimizer_AdamW(\n",
    "            betas=(0.99, 0.99), \n",
    "            weight_decay=1e-2), \n",
    "\n",
    "        Best epochs:\n",
    "            [4, 7, 3, 3, 1, 0, 0, 0, 0, 0]\n",
    "        Average training epochs for best model: 1.8\n",
    "    \n",
    "        Cumulative confusion:\n",
    "            [[254   4]\n",
    "            [  4  81]]\n",
    "        Acc 97.67% F1 0.953\n",
    "\n",
    "    Task 2:\n",
    "\n",
    "    Task 3:\n",
    "\"\"\"\n",
    "\n",
    "parameters = (0.0, 500, 0.001, 32, 5)\n",
    "for task in range(1, 4):\n",
    "    accuracy, f1, confusion = evaluate(data, parameters, task)\n",
    "    print(f\"Acc: {round(accuracy*100,2)}%\\n f1: {round(f1,3)}\")\n",
    "    print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Task 2 and 3 are not yet implemented",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m accuracy, f1, confusion \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(accuracy\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m f1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(f1,\u001b[38;5;241m3\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(confusion)\n",
      "Cell \u001b[1;32mIn[41], line 4\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(params, task)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(params, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask argument must be either 1, 2 or 3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m task \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask 2 and 3 are not yet implemented\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m     dropout, hidden_size, learning_rate, batch_size, n_hidden \u001b[38;5;241m=\u001b[39m params\n\u001b[0;32m      8\u001b[0m     max_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Task 2 and 3 are not yet implemented"
     ]
    }
   ],
   "source": [
    "accuracy, f1, confusion = evaluate(data=data, parameters=params, task=2)\n",
    "print(f\"acc: {round(accuracy*100,2)}%\\n f1: {round(f1,3)}\")\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy, f1, confusion = evaluate(data=data, parameters=params, task=3)\n",
    "print(f\"acc: {round(accuracy*100,2)}%\\n f1: {round(f1,3)}\")\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_nn ={\n",
    "#     'dropout': [0.5],\n",
    "#     'hidden_size': list(range(500, 501, 100)),\n",
    "#     'learning_rate':[0.01, 0.001, 0.0001, 0.00001],\n",
    "#     'batch_size':[8, 16, 32, 64, 128],\n",
    "#     'n_hidden': list(range(1, 4, 1))\n",
    "# }\n",
    "# parameter_expansion = list(product(*params_nn.values()))\n",
    "# print(len(parameter_expansion))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
