{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hi :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RECORDING_SESSION_LABEL</th>\n",
       "      <th>trial</th>\n",
       "      <th>IA_ID</th>\n",
       "      <th>item</th>\n",
       "      <th>list</th>\n",
       "      <th>IA_LABEL</th>\n",
       "      <th>wordlength</th>\n",
       "      <th>condition</th>\n",
       "      <th>is_critical</th>\n",
       "      <th>is_spill1</th>\n",
       "      <th>...</th>\n",
       "      <th>duration_firstpass</th>\n",
       "      <th>duration_firstfixation</th>\n",
       "      <th>fix_count</th>\n",
       "      <th>avg_pupil</th>\n",
       "      <th>IA_REGRESSION_IN_COUNT</th>\n",
       "      <th>IA_REGRESSION_OUT_COUNT</th>\n",
       "      <th>saccade_length</th>\n",
       "      <th>saccade_duration</th>\n",
       "      <th>go_past_time</th>\n",
       "      <th>sentenceCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10m23r2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Viel</td>\n",
       "      <td>4</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>236</td>\n",
       "      <td>236</td>\n",
       "      <td>1</td>\n",
       "      <td>1408.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72.376792</td>\n",
       "      <td>16</td>\n",
       "      <td>236</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10m23r2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Geld</td>\n",
       "      <td>4</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>424</td>\n",
       "      <td>264</td>\n",
       "      <td>3</td>\n",
       "      <td>1379.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>71.519648</td>\n",
       "      <td>160</td>\n",
       "      <td>424</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10m23r2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>wurde</td>\n",
       "      <td>5</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10m23r2</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>investiert,</td>\n",
       "      <td>11</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>420</td>\n",
       "      <td>268</td>\n",
       "      <td>3</td>\n",
       "      <td>1290.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>65.401223</td>\n",
       "      <td>12</td>\n",
       "      <td>420</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10m23r2</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>bevor</td>\n",
       "      <td>5</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>296</td>\n",
       "      <td>296</td>\n",
       "      <td>2</td>\n",
       "      <td>1242.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>42.311819</td>\n",
       "      <td>12</td>\n",
       "      <td>296</td>\n",
       "      <td>control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11171</th>\n",
       "      <td>9m23r1</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>Verteidigung</td>\n",
       "      <td>12</td>\n",
       "      <td>filler</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>168</td>\n",
       "      <td>168</td>\n",
       "      <td>2</td>\n",
       "      <td>497.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>91.810675</td>\n",
       "      <td>20</td>\n",
       "      <td>168</td>\n",
       "      <td>filler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11172</th>\n",
       "      <td>9m23r1</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>Europas</td>\n",
       "      <td>7</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>224</td>\n",
       "      <td>224</td>\n",
       "      <td>1</td>\n",
       "      <td>493.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>119.788355</td>\n",
       "      <td>24</td>\n",
       "      <td>224</td>\n",
       "      <td>filler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11173</th>\n",
       "      <td>9m23r1</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>erhöht</td>\n",
       "      <td>6</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>332</td>\n",
       "      <td>332</td>\n",
       "      <td>1</td>\n",
       "      <td>472.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>95.117033</td>\n",
       "      <td>20</td>\n",
       "      <td>332</td>\n",
       "      <td>filler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11174</th>\n",
       "      <td>9m23r1</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>werden</td>\n",
       "      <td>6</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>244</td>\n",
       "      <td>244</td>\n",
       "      <td>1</td>\n",
       "      <td>477.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58.649126</td>\n",
       "      <td>16</td>\n",
       "      <td>244</td>\n",
       "      <td>filler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11175</th>\n",
       "      <td>9m23r1</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>sollten.</td>\n",
       "      <td>8</td>\n",
       "      <td>none</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>495.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1164</td>\n",
       "      <td>filler</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11176 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      RECORDING_SESSION_LABEL  trial  IA_ID  item  list      IA_LABEL  \\\n",
       "0                     10m23r2     12      1     1     2          Viel   \n",
       "1                     10m23r2     12      2     1     2          Geld   \n",
       "2                     10m23r2     12      3     1     2         wurde   \n",
       "3                     10m23r2     12      4     1     2   investiert,   \n",
       "4                     10m23r2     12      5     1     2         bevor   \n",
       "...                       ...    ...    ...   ...   ...           ...   \n",
       "11171                  9m23r1     10     12    28     1  Verteidigung   \n",
       "11172                  9m23r1     10     13    28     1       Europas   \n",
       "11173                  9m23r1     10     14    28     1        erhöht   \n",
       "11174                  9m23r1     10     15    28     1        werden   \n",
       "11175                  9m23r1     10     16    28     1      sollten.   \n",
       "\n",
       "       wordlength condition  is_critical  is_spill1  ...  duration_firstpass  \\\n",
       "0               4      none            0          0  ...                 236   \n",
       "1               4      none            0          0  ...                 424   \n",
       "2               5      none            0          0  ...                   0   \n",
       "3              11      none            0          0  ...                 420   \n",
       "4               5      none            0          0  ...                 296   \n",
       "...           ...       ...          ...        ...  ...                 ...   \n",
       "11171          12    filler            0          0  ...                 168   \n",
       "11172           7      none            0          0  ...                 224   \n",
       "11173           6      none            0          0  ...                 332   \n",
       "11174           6      none            0          0  ...                 244   \n",
       "11175           8      none            0          0  ...                 160   \n",
       "\n",
       "       duration_firstfixation  fix_count    avg_pupil  IA_REGRESSION_IN_COUNT  \\\n",
       "0                         236          1  1408.000000                       0   \n",
       "1                         264          3  1379.333333                       2   \n",
       "2                           0          0     0.000000                       0   \n",
       "3                         268          3  1290.000000                       1   \n",
       "4                         296          2  1242.500000                       1   \n",
       "...                       ...        ...          ...                     ...   \n",
       "11171                     168          2   497.000000                       0   \n",
       "11172                     224          1   493.000000                       0   \n",
       "11173                     332          1   472.000000                       0   \n",
       "11174                     244          1   477.000000                       0   \n",
       "11175                     160          1   495.000000                       0   \n",
       "\n",
       "       IA_REGRESSION_OUT_COUNT  saccade_length  saccade_duration  \\\n",
       "0                            0       72.376792                16   \n",
       "1                            0       71.519648               160   \n",
       "2                            1        0.000000                 0   \n",
       "3                            0       65.401223                12   \n",
       "4                            0       42.311819                12   \n",
       "...                        ...             ...               ...   \n",
       "11171                        0       91.810675                20   \n",
       "11172                        0      119.788355                24   \n",
       "11173                        0       95.117033                20   \n",
       "11174                        0       58.649126                16   \n",
       "11175                        1        0.000000                 0   \n",
       "\n",
       "       go_past_time  sentenceCondition  \n",
       "0               236            control  \n",
       "1               424            control  \n",
       "2                 0            control  \n",
       "3               420            control  \n",
       "4               296            control  \n",
       "...             ...                ...  \n",
       "11171           168             filler  \n",
       "11172           224             filler  \n",
       "11173           332             filler  \n",
       "11174           244             filler  \n",
       "11175          1164             filler  \n",
       "\n",
       "[11176 rows x 29 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import product # used for hyperparameter grid search, unused if not doing hyperparameter tuning\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seed = 42 # for reproducibility\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10m23r2' '11w20l3' '13w53r1' '14w19l2' '15w20r3' '16m20l4' '17w24r1'\n",
      " '18w19l2' '19w21l3' '20w20r4' '21m29l1' '22m23r2' '23w24r3' '24w22r4'\n",
      " '25m23l1' '26m18l2' '28m3r4' '29w22r1' '2m22r2' '30w22r4' '31m20r4'\n",
      " '32w22l1' '33w35r3' '34m23r3' '35m24l2' '3m25r3' '4f21r4' '5f22r1'\n",
      " '6m129r2' '7m21l3' '8m22l4' '9m23r1']\n"
     ]
    }
   ],
   "source": [
    "print(data[\"RECORDING_SESSION_LABEL\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.features.iloc[index].to_numpy()\n",
    "        label = self.labels.iloc[index]\n",
    "        return features, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, batch_size, task, alpha):\n",
    "    \n",
    "    dataloaders = []\n",
    "    pos_weights = []\n",
    "    # global class_weights\n",
    "\n",
    "    if task == 0 or task == 1: # Known subjects and items\n",
    "        features, labels = data\n",
    "        dataset = CustomDataset(features, labels)\n",
    "        n = len(dataset)\n",
    "        if task == 0:\n",
    "            k = 10 # k-fold cross-validation\n",
    "        elif task == 1:\n",
    "            k = n # leave-one-out cross-validation\n",
    "        fold_size = n // k\n",
    "        folds = []\n",
    "        for i in range(k):\n",
    "            start = i * fold_size\n",
    "            end = (i + 1) * fold_size if i < k - 1 else n\n",
    "            folds.append(torch.utils.data.Subset(dataset, range(start, end)))\n",
    "\n",
    "        for i in range(k):\n",
    "            # splits for cross-validation, validation set = test set (since we're doing k-fold, we won't use a separate test set)\n",
    "            validation_dataset = folds[i]\n",
    "            t = i + 1 if i < k - 1 else 0\n",
    "            test_dataset = folds[t]\n",
    "            train_folds = [folds[j] for j in range(k) if j != i]# and j != t]\n",
    "            train_dataset = torch.utils.data.ConcatDataset(train_folds)\n",
    "\n",
    "            # class weights for weighted cross-entropy loss (to handle class imbalance)\n",
    "            y = torch.tensor([label for _, label in train_dataset], dtype=torch.long)\n",
    "            sum = y.sum().item()\n",
    "            weight = alpha*(len(train_dataset)-sum) / sum\n",
    "            pos_weights.append(torch.tensor(weight, dtype=torch.float))\n",
    "\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "            test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "            dataloaders.append((train_dataloader, validation_dataloader, test_dataloader))\n",
    "            #dataloaders.append((train_dataloader, validation_dataloader))\n",
    "\n",
    "        return dataloaders, pos_weights\n",
    "    \n",
    "    elif task == 2: # Held-out subjects, known items\n",
    "        test_items_count = 0\n",
    "        subject_ids = list(data.groups.keys())\n",
    "\n",
    "        for i in range(0, len(subject_ids), 2):\n",
    "            subject_indexes = [i, i+1]\n",
    "            subjects = [subject_ids[subject_index] for subject_index in subject_indexes]\n",
    "\n",
    "            others = [subject for j, subject in enumerate(subject_ids) if j != i and j != i+1]\n",
    "\n",
    "            test = pd.concat([data.get_group(subject) for subject in subjects])\n",
    "            test_items_count += len(test)\n",
    "            train_eval = pd.concat([data.get_group(subject) for subject in others])\n",
    "            shuffled = train_eval.sample(frac = 1, random_state=seed) # shuffle the data -> wrecked.\n",
    "            \n",
    "            # splitting data into features and labels for dataset creation\n",
    "            test_labels = test[\"condition\"].copy()\n",
    "            test_features = test.copy().drop([\"condition\", \"sentenceCondition\", \"RECORDING_SESSION_LABEL\", \"item\"], axis=1)\n",
    "            test_dataset = CustomDataset(test_features, test_labels)\n",
    "            \n",
    "            train_eval_labels = shuffled[\"condition\"].copy()\n",
    "            train_eval_features = shuffled.copy().drop([\"condition\", \"sentenceCondition\", \"RECORDING_SESSION_LABEL\", \"item\"], axis=1)\n",
    "            train_eval_dataset = CustomDataset(train_eval_features, train_eval_labels)\n",
    "\n",
    "\n",
    "            train_eval_split = 0.9\n",
    "            train_size = int(train_eval_split * len(train_eval_dataset))\n",
    "            validation_size = len(train_eval_dataset) - train_size\n",
    "            train_dataset, validation_dataset = torch.utils.data.random_split(train_eval_dataset, [train_size, validation_size])\n",
    "\n",
    "            # class weights for weighted cross-entropy loss (to handle class imbalance)\n",
    "            y = torch.tensor([label for _, label in train_dataset], dtype=torch.long)\n",
    "            sum = y.sum().item()\n",
    "            weight = alpha*(len(train_dataset)-sum) / sum\n",
    "            pos_weights.append(torch.tensor(weight, dtype=torch.float))\n",
    "\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "            test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "            dataloaders.append((train_dataloader, validation_dataloader, test_dataloader))\n",
    "        return dataloaders, pos_weights\n",
    "    \n",
    "    elif task == 3: # Held-out items, known subjects\n",
    "        test_items_count = 0\n",
    "        for i, item in enumerate(data.groups.keys()):\n",
    "            test = data.get_group(item)\n",
    "            test_items_count += len(test)\n",
    "            train_eval = pd.concat([data.get_group(i) for i in data.groups.keys() if i != item])\n",
    "            shuffled = train_eval.sample(frac = 1, random_state=seed) # shuffle the data -> wrecked.\n",
    "            \n",
    "            # splitting data into features and labels for dataset creation\n",
    "            test_labels = test[\"condition\"].copy()\n",
    "            test_features = test.copy().drop([\"condition\", \"sentenceCondition\", \"RECORDING_SESSION_LABEL\", \"item\"], axis=1)\n",
    "            test_dataset = CustomDataset(test_features, test_labels)\n",
    "            \n",
    "            train_eval_labels = shuffled[\"condition\"].copy()\n",
    "            train_eval_features = shuffled.copy().drop([\"condition\", \"sentenceCondition\", \"RECORDING_SESSION_LABEL\", \"item\"], axis=1)\n",
    "            train_eval_dataset = CustomDataset(train_eval_features, train_eval_labels)\n",
    "\n",
    "\n",
    "            train_eval_split = 0.9\n",
    "            train_size = int(train_eval_split * len(train_eval_dataset))\n",
    "            validation_size = len(train_eval_dataset) - train_size\n",
    "            train_dataset, validation_dataset = torch.utils.data.random_split(train_eval_dataset, [train_size, validation_size])\n",
    "\n",
    "            # class weights for weighted cross-entropy loss (to handle class imbalance)\n",
    "            y = torch.tensor([label for _, label in train_dataset], dtype=torch.long)\n",
    "            sum = y.sum().item()\n",
    "            weight = alpha*(len(train_dataset)-sum) / sum\n",
    "            pos_weights.append(torch.tensor(weight, dtype=torch.float))\n",
    "\n",
    "            train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "            test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "            dataloaders.append((train_dataloader, validation_dataloader, test_dataloader))\n",
    "        return dataloaders, pos_weights\n",
    "    else:\n",
    "        raise ValueError(\"Task argument must be either 1, 2, or 3\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_split_data(data, batch_size, task, alpha):\n",
    "    \n",
    "# all tasks\n",
    "    \n",
    "    data_copy = data.loc[data['is_critical'] == 1].copy()\n",
    "    dropped = data_copy.drop(['composite', 'LF', 'HF', \"IA_ID\", \"trial\", \"list\", \"IA_LABEL\", \"wordlength\", \"is_critical\", \n",
    "                'is_spill1', 'is_spill2', 'is_spill3', 'filler', 'function_word', 'other_filler'], axis=1)\n",
    "\n",
    "    # normalizing input features beforehand, increased performance vs adding batchnorm layer to model\n",
    "    temp = dropped[['fixation_duration',\n",
    "        'duration_firstpass', 'duration_firstfixation', 'fix_count',\n",
    "        'avg_pupil', 'IA_REGRESSION_IN_COUNT', 'IA_REGRESSION_OUT_COUNT',\n",
    "        'saccade_length', 'saccade_duration', 'go_past_time']]\n",
    "    temp = (temp - temp.mean()) / temp.std()\n",
    "    dropped[['fixation_duration',\n",
    "        'duration_firstpass', 'duration_firstfixation', 'fix_count',\n",
    "        'avg_pupil', 'IA_REGRESSION_IN_COUNT', 'IA_REGRESSION_OUT_COUNT',\n",
    "        'saccade_length', 'saccade_duration', 'go_past_time']] = temp\n",
    "    normalized = dropped\n",
    "    # mapping condition and sentenceCondition to 0 and 1 for critical word classification\n",
    "    normalized[[\"condition\", \"sentenceCondition\"]] = normalized[[\"condition\", \"sentenceCondition\"]].map(lambda x: x.replace(\"none\", \"0\"))\n",
    "    normalized[[\"condition\", \"sentenceCondition\"]] = normalized[[\"condition\", \"sentenceCondition\"]].map(lambda x: x.replace(\"control\", \"0\"))\n",
    "    normalized[[\"condition\", \"sentenceCondition\"]] = normalized[[\"condition\", \"sentenceCondition\"]].map(lambda x: x.replace(\"pseudo\", \"1\"))\n",
    "    normalized[[\"condition\", \"sentenceCondition\"]] = normalized[[\"condition\", \"sentenceCondition\"]].map(lambda x: x.replace(\"filler\", \"0\"))\n",
    "    normalized[[\"condition\", \"sentenceCondition\"]] = normalized[[\"condition\", \"sentenceCondition\"]].astype(int)\n",
    "    mapped = normalized\n",
    "\n",
    "# task 1 specific steps\n",
    "    if task == 0 or task == 1: # Known subjects and items\n",
    "        shuffled = mapped.sample(frac = 1, random_state=seed) # shuffle the data -> wrecked.\n",
    "        # splitting data into features and labels for dataset creation\n",
    "        labels = shuffled[\"condition\"].copy()\n",
    "        features = shuffled.copy().drop([\"condition\", \"sentenceCondition\", \"RECORDING_SESSION_LABEL\", \"item\"], axis=1)\n",
    "        data = (features, labels)\n",
    "        return split_data(data, batch_size, task, alpha)\n",
    "    \n",
    "    elif task == 2: # Held-out subjects, known items\n",
    "        subjects = mapped.groupby('RECORDING_SESSION_LABEL')\n",
    "        return split_data(subjects, batch_size, task, alpha)\n",
    "    elif task == 3: # Held-out items, known subjects\n",
    "        items = mapped.groupby('item')\n",
    "        return split_data(items, batch_size, task, alpha)\n",
    "    else:\n",
    "        raise ValueError(\"Task argument must be either 1, 2, or 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(model, dataloader, optimizer, pos_weight, training=\"train\"):\n",
    "   \n",
    "    loss_function = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    if training == \"train\":\n",
    "        model.train()\n",
    "    elif training == \"validation\":\n",
    "        model.eval()\n",
    "    elif training == \"test\":\n",
    "        model.eval()\n",
    "    else:\n",
    "        raise ValueError(\"training argument must be either 'train', 'validation' or 'test'\")\n",
    "        \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    cumulative_loss = 0\n",
    "    prediction_list = []\n",
    "    label_list = []\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    for sample in dataloader:\n",
    "   \n",
    "        data, targets = sample[0].float().to(device), sample[1].type(torch.LongTensor).to(device)\n",
    "        output = model(data)\n",
    "        loss_value = loss_function(output, targets.unsqueeze(1).float())\n",
    "        cumulative_loss += loss_value.item()\n",
    "\n",
    "        if training == \"train\":\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        predictions = [round(x) for x in sigmoid(output).to('cpu').detach().squeeze(1).numpy().tolist()]#.argmax(axis=1) # was necessary for multi-class classification\n",
    "        target_labels = targets.to('cpu').detach().numpy()\n",
    "        total += len(predictions)\n",
    "        correct += accuracy_score(target_labels, predictions, normalize=False)\n",
    "        prediction_list.extend(predictions)\n",
    "        label_list.extend(target_labels)\n",
    "    if training == \"test\":\n",
    "        return label_list, prediction_list\n",
    "    f1 = f1_score(label_list, prediction_list)\n",
    "    accuracy = accuracy_score(label_list, prediction_list)\n",
    "    confusion = confusion_matrix(label_list, prediction_list)\n",
    "\n",
    "    return cumulative_loss, accuracy, f1, confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TuneableModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, layer_size, dropout_rate, n_layers):\n",
    "        super(TuneableModel, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.input_layer = torch.nn.LazyLinear(layer_size) # input_size inferred from input tensor\n",
    "        self.linear2 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear3 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear4 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear5 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear6 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear7 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear8 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear9 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear10 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.output_layer = torch.nn.Linear(layer_size, 1)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.activation = torch.nn.LeakyReLU()\n",
    "        self.batchnorm = torch.nn.BatchNorm1d(layer_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        #x = self.batchnorm(x) # batchnorm layer didn't improve performance\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        if self.n_layers > 1:\n",
    "            x = self.linear2(x)\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout(x)\n",
    "            if self.n_layers > 2:\n",
    "                x = self.linear3(x)\n",
    "                x = self.activation(x)\n",
    "                x = self.dropout(x)\n",
    "                if self.n_layers > 3:\n",
    "                    x = self.linear4(x)\n",
    "                    x = self.activation(x)\n",
    "                    x = self.dropout(x)\n",
    "                    if self.n_layers > 4:\n",
    "                        x = self.linear5(x)\n",
    "                        x = self.activation(x)\n",
    "                        x = self.dropout(x)\n",
    "                        if self.n_layers > 5:\n",
    "                            x = self.linear6(x)\n",
    "                            x = self.activation(x)\n",
    "                            x = self.dropout(x)\n",
    "                            if self.n_layers > 6:\n",
    "                                x = self.linear7(x)\n",
    "                                x = self.activation(x)\n",
    "                                x = self.dropout(x)\n",
    "                                if self.n_layers > 7:\n",
    "                                    x = self.linear8(x)\n",
    "                                    x = self.activation(x)\n",
    "                                    x = self.dropout(x)\n",
    "                                    if self.n_layers > 8:\n",
    "                                        x = self.linear9(x)\n",
    "                                        x = self.activation(x)\n",
    "                                        x = self.dropout(x)\n",
    "                                        if self.n_layers > 9:\n",
    "                                            x = self.linear10(x)\n",
    "                                            x = self.activation(x)\n",
    "                                            x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training sample\n",
    "def evaluate(data, parameters, task):\n",
    "    assert task in [0, 1, 2, 3], \"Task argument must be either 1, 2 or 3\"\n",
    "    \n",
    "    try:\n",
    "        pos_weight = torch.tensor(parameters[\"pos_weight\"], dtype=torch.float).to(device)\n",
    "    except:\n",
    "        pos_weight = None\n",
    "\n",
    "    max_epochs = 1000\n",
    "\n",
    "    dataloaders, class_weights = preprocess_and_split_data(data, parameters[\"batch_size\"], task, alpha=parameters[\"alpha\"])\n",
    "\n",
    "    input_size = 10 # number of features :( -> this is hardcoded for now, try to get it from the dataset\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    torch.manual_seed(seed)\n",
    "    model = TuneableModel(input_size, parameters[\"hidden_size\"], parameters[\"dropout\"], parameters[\"n_hidden\"])\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=parameters[\"learning_rate\"], \n",
    "                                  betas=(parameters[\"beta_1\"], parameters[\"beta_2\"]), weight_decay=1e-2)\n",
    "\n",
    "\n",
    "    \n",
    "    for i, dataloader in tqdm(enumerate(dataloaders)):\n",
    "        pos_weight = class_weights[i] if pos_weight is None else pos_weight\n",
    "        max_patience = 10 if i < 35 else 2\n",
    "        last_loss = 1000000\n",
    "        best_epoch = 0\n",
    "        PATH = f\"model_{i}.pt\"\n",
    "        train_dataloader, validation_dataloader, test_dataloader = dataloader[0], dataloader[1], dataloader[2]\n",
    "        for epoch in range(max_epochs):\n",
    "            # training\n",
    "            train_loss, train_accuracy, train_f1, train_confusion = train_test(model, train_dataloader, optimizer, pos_weight, training=\"train\")\n",
    "            train_loss, train_accuracy, train_f1 = round(train_loss, 2), round(train_accuracy, 4), round(train_f1, 2)\n",
    "            # validation at end of epoch\n",
    "            validation_loss, validation_accuracy, validation_f1, validation_confusion = train_test(model, validation_dataloader, optimizer, pos_weight, training=\"validation\")\n",
    "            validation_loss, validation_accuracy, validation_f1 = round(validation_loss, 2), round(validation_accuracy, 4), round(validation_f1, 2)\n",
    "            if validation_loss < last_loss:\n",
    "                last_loss = validation_loss\n",
    "                best_epoch = epoch\n",
    "                current_patience = 0\n",
    "            else:\n",
    "                if current_patience == 0:\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': last_loss,\n",
    "                        }, PATH)\n",
    "                current_patience += 1\n",
    "            if current_patience == max_patience:\n",
    "                break   \n",
    "\n",
    "        # Testing once patience is reached\n",
    "        torch.manual_seed(seed)\n",
    "        model = TuneableModel(input_size, parameters[\"hidden_size\"], parameters[\"dropout\"], parameters[\"n_hidden\"])\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=parameters[\"learning_rate\"], betas=(0.99, 0.99), weight_decay=1e-4)\n",
    "        checkpoint = torch.load(PATH)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        prediction_list, label_list = train_test(model, test_dataloader, optimizer, pos_weight, training=\"test\")\n",
    "        predictions.extend(prediction_list)\n",
    "        labels.extend(label_list)\n",
    "\n",
    "    return accuracy_score(labels, predictions), f1_score(labels, predictions), confusion_matrix(labels, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Parameters:\n",
    "    Patience = 10\n",
    "    @ model_params(\n",
    "        dropout: 0.0\n",
    "        layer size: 500\n",
    "        lr: 0.001\n",
    "        batch_size: 16\n",
    "        n_layers: 6)\n",
    "    @ optimizer_AdamW(\n",
    "        beta_1: 0.999,\n",
    "        beta_2: 0.999, \n",
    "        weight_decay: 1e-2)\n",
    "    @ BCEWithLogitsLoss(\n",
    "        pos_weight: vairable)\n",
    "\n",
    "### Performance:\n",
    "\n",
    "    Task 1: Known subjects, known items\n",
    "\n",
    "        Train:Eval:Test - 80:10:10 - 10-fold\n",
    "            @ alpha = 0.21\n",
    "        Acc: 98.25%\n",
    "        F1: 0.9643\n",
    "        Confusion:\n",
    "            [[256   4]\n",
    "            [  2  81]]\n",
    "\n",
    "        Blind LOOCV: Train:Eval:Test - n-2:1:1\n",
    "            @ alpha = 0.21\n",
    "        Acc: 100.0%\n",
    "        F1: 1.0\n",
    "        Confusion:\n",
    "            [[258   0]\n",
    "            [  0  85]]\n",
    "            \n",
    "    Task 2: Left-out subjects, known items\n",
    "    \n",
    "        Train:eval:test - 90:10:(1 subject) <- sadface\n",
    "            @ alpha = 0.21\n",
    "        Acc: 98.54%\n",
    "        F1: 0.9701\n",
    "        Confusion:\n",
    "            [[257   4]\n",
    "            [  1  81]]\n",
    "        \n",
    "        Train:eval:test - 90:10:(2 subjects)\n",
    "            @ alpha = 0.21\n",
    "        Acc: 97.08%\n",
    "        F1: 0.9405\n",
    "        Confusion:\n",
    "            [[254   6]\n",
    "            [  4  79]]\n",
    "\n",
    "    Task 3: Left-out items, known subjects, train:eval:test - 90:10:item\n",
    "            @ pos_weight = 0.8, dropout =0.01\n",
    "        Acc: 96.5%\n",
    "        F1: 0.9294\n",
    "        Confusion:\n",
    "            [[252   6]\n",
    "            [  6  79]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda with parameters: \n",
      "@ model_params(\n",
      "\tdropout=0.0,      \n",
      "\thidden_size=500,\n",
      "\tlearning_rate=0.001,      \n",
      "\tbatch_size=16,\n",
      "\tn_hidden=6),\n",
      "@ optimizer_AdamW(\n",
      "\tbeta_1=0.999,\n",
      "\tbeta_2=0.999)\n",
      "@ BCEWithLogitsLoss(\n",
      "\talpha=0.21)\n",
      "\n",
      "Task: Known subjects and items, 10-fold CV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:23,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 98.25%\n",
      "F1: 0.9643\n",
      "Confusion:\n",
      " [[256   4]\n",
      " [  2  81]]\n",
      "Training on cuda with parameters: \n",
      "@ model_params(\n",
      "\tdropout=0.0,      \n",
      "\thidden_size=500,\n",
      "\tlearning_rate=0.001,      \n",
      "\tbatch_size=16,\n",
      "\tn_hidden=6),\n",
      "@ optimizer_AdamW(\n",
      "\tbeta_1=0.999,\n",
      "\tbeta_2=0.999)\n",
      "@ BCEWithLogitsLoss(\n",
      "\talpha=0.21)\n",
      "\n",
      "Task: Known subjects and items, LOOCV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:26,  2.17s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "parameters = {\n",
    "    \"dropout\": 0.0,\n",
    "    \"hidden_size\": 500,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"batch_size\": 16,\n",
    "    \"n_hidden\": 6,\n",
    "    \"beta_1\": 0.999,\n",
    "    \"beta_2\": 0.999,\n",
    "    \"alpha\": 0.21\n",
    "}\n",
    "\n",
    "\n",
    "tasks = [\"Known subjects and items, 10-fold CV\", \"Known subjects and items, LOOCV\", \"2 Held-out subjects, known items\", \"1 Held-out items, known subjects\"]\n",
    "\n",
    "for task in [0, 1, 2, 3]:\n",
    "    if task == 0:\n",
    "        parameters[\"alpha\"] = 0.21 # 0.21 best so far\n",
    "        parameters[\"pos_weight\"] = None\n",
    "    elif task == 1:\n",
    "        parameters[\"alpha\"] = 0.21\n",
    "        parameters[\"pos_weight\"] = None\n",
    "    elif task == 2:\n",
    "        parameters[\"alpha\"] = 0.19\n",
    "        parameters[\"pos_weight\"] = None\n",
    "    elif task == 3:\n",
    "        parameters[\"alpha\"] = 1\n",
    "        parameters[\"pos_weight\"] = .8\n",
    "        parameters[\"dropout\"] = 0.01 # dumb.\n",
    "        parameters[\"beta_2\"] = 0.99\n",
    "\n",
    "    print(f\"Task: {tasks[task]}\")\n",
    "    print(f'Training on {device} with parameters: \\n@ model_params(\\n\\tdropout={parameters[\"dropout\"]},\\\n",
    "      \\n\\thidden_size={parameters[\"hidden_size\"]},\\n\\tlearning_rate={parameters[\"learning_rate\"]},\\\n",
    "      \\n\\tbatch_size={parameters[\"batch_size\"]},\\n\\tn_hidden={parameters[\"n_hidden\"]}),')\n",
    "    print(f'@ optimizer_AdamW(\\n\\tbeta_1={parameters[\"beta_1\"]},\\n\\tbeta_2={parameters[\"beta_2\"]})')\n",
    "    print(f'@ BCEWithLogitsLoss(\\n\\talpha={parameters[\"alpha\"]})\\n')\n",
    "    accuracy, f1, confusion = evaluate(data, parameters, task)\n",
    "    print(f\"Acc: {round(accuracy*100,2)}%\\nF1: {round(f1,4)}\")\n",
    "    print(\"Confusion:\\n\", confusion, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
