{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hi :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f2767b2bf0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seed = 42\n",
    "\n",
    "data = pd.read_csv('data.csv', delimiter=';')\n",
    "data.keys()\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dropped = data.loc[data['is_critical'] == 1].copy()\n",
    "dropped.drop(['composite', 'LF', 'HF', \"RECORDING_SESSION_LABEL\", \"trial\", \"IA_ID\", \"item\", \"list\", \"IA_LABEL\", \"wordlength\", \"is_critical\", \n",
    "              'is_spill1', 'is_spill2', 'is_spill3', 'filler', 'function_word', 'other_filler'], axis=1, inplace=True)\n",
    "print(len(dropped))\n",
    "dropped[[\"condition\", \"sentenceCondition\"]] = dropped[[\"condition\", \"sentenceCondition\"]].map(lambda x: x.replace(\"none\", \"0\"))\n",
    "dropped[[\"condition\", \"sentenceCondition\"]] = dropped[[\"condition\", \"sentenceCondition\"]].map(lambda x: x.replace(\"control\", \"0\"))\n",
    "dropped[[\"condition\", \"sentenceCondition\"]] = dropped[[\"condition\", \"sentenceCondition\"]].map(lambda x: x.replace(\"pseudo\", \"1\"))\n",
    "dropped[[\"condition\", \"sentenceCondition\"]] = dropped[[\"condition\", \"sentenceCondition\"]].map(lambda x: x.replace(\"filler\", \"0\"))\n",
    "\n",
    "normalized = dropped[['fixation_duration',\n",
    "       'duration_firstpass', 'duration_firstfixation', 'fix_count',\n",
    "       'avg_pupil', 'IA_REGRESSION_IN_COUNT', 'IA_REGRESSION_OUT_COUNT',\n",
    "       'saccade_length', 'saccade_duration', 'go_past_time']]\n",
    "normalized = (normalized - normalized.mean()) / normalized.std()\n",
    "dropped[['fixation_duration',\n",
    "       'duration_firstpass', 'duration_firstfixation', 'fix_count',\n",
    "       'avg_pupil', 'IA_REGRESSION_IN_COUNT', 'IA_REGRESSION_OUT_COUNT',\n",
    "       'saccade_length', 'saccade_duration', 'go_past_time']] = normalized\n",
    "\n",
    "\n",
    "labels = dropped[\"condition\"].copy()\n",
    "labels = labels.astype('int')\n",
    "#dropped[['LF', 'HF']] = dropped[['LF', 'HF']].astype('int')\n",
    "\n",
    "features = dropped.copy().drop([\"condition\", \"sentenceCondition\"], axis=1)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        features = self.features.iloc[index].to_numpy()\n",
    "        label = self.labels.iloc[index]\n",
    "        return features, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "dataset = CustomDataset(features=features, labels=labels)\n",
    "input_size = len(features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_split_data(dataset, batch_size, k=5):\n",
    "    n = len(dataset)\n",
    "    fold_size = n // k\n",
    "    folds = []\n",
    "    for i in range(k):\n",
    "        start = i * fold_size\n",
    "        end = (i + 1) * fold_size if i < k - 1 else n\n",
    "        folds.append(torch.utils.data.Subset(dataset, range(start, end)))\n",
    "\n",
    "    dataloaders = []\n",
    "    for i in range(k):\n",
    "        validation_dataset = folds[i]\n",
    "        train_folds = [folds[j] for j in range(k) if j != i]\n",
    "        train_dataset = torch.utils.data.ConcatDataset(train_folds)\n",
    "\n",
    "        y = torch.tensor([label for _, label in train_dataset], dtype=torch.long)\n",
    "\n",
    "        global class_weights\n",
    "        class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y), y=y.numpy())\n",
    "        class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=True)\n",
    "        dataloaders.append((train_dataloader, validation_dataloader))\n",
    "\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(model, dataloader, optimizer, training=\"train\"):\n",
    "   \n",
    "    loss_function = torch.nn.BCEWithLogitsLoss()#weight=class_weights.to(device))\n",
    "\n",
    "    if training == \"train\":\n",
    "        model.train()\n",
    "    elif training == \"validation\":\n",
    "        model.eval()\n",
    "    elif training == \"test\":\n",
    "        model.eval()\n",
    "    else:\n",
    "        raise ValueError(\"training argument must be either 'train', 'validation' or 'test'\")\n",
    "        \n",
    "    total = 0\n",
    "    correct = 0\n",
    "    cumulative_loss = 0\n",
    "    prediction_list = []\n",
    "    label_list = []\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    for sample in dataloader:\n",
    "   \n",
    "        data, targets = sample[0].float().to(device), sample[1].type(torch.LongTensor).to(device)\n",
    "        output = model(data)\n",
    "        loss_value = loss_function(output, targets.unsqueeze(1).float())\n",
    "        cumulative_loss += loss_value.item()\n",
    "\n",
    "        if training == \"train\":\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        predictions = [round(x) for x in sigmoid(output).to('cpu').detach().squeeze(1).numpy().tolist()]#.argmax(axis=1)\n",
    "        target_labels = targets.to('cpu').detach().numpy()\n",
    "        total += len(predictions)\n",
    "        correct += accuracy_score(target_labels, predictions, normalize=False)\n",
    "        prediction_list.extend(predictions)\n",
    "        label_list.extend(target_labels)\n",
    "    if training == \"test\":\n",
    "        return label_list, prediction_list\n",
    "    f1 = f1_score(label_list, prediction_list)\n",
    "    accuracy = accuracy_score(label_list, prediction_list)\n",
    "    confusion = confusion_matrix(label_list, prediction_list)\n",
    "\n",
    "    return cumulative_loss, accuracy, f1, confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TuneableModel(torch.nn.Module):\n",
    "    def __init__(self, layer_size, dropout_rate, n_layers):\n",
    "        super(TuneableModel, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.input_layer = torch.nn.Linear(input_size, layer_size)\n",
    "        self.linear2 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear3 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear4 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear5 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear6 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear7 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear8 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear9 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.linear10 = torch.nn.Linear(layer_size, layer_size)\n",
    "        self.output_layer = torch.nn.Linear(layer_size, 1)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.activation = torch.nn.LeakyReLU()\n",
    "        self.batchnorm = torch.nn.BatchNorm1d(layer_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        #x = self.batchnorm(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        if self.n_layers > 1:\n",
    "            x = self.linear2(x)\n",
    "            x = self.activation(x)\n",
    "            x = self.dropout(x)\n",
    "            if self.n_layers > 2:\n",
    "                x = self.linear3(x)\n",
    "                x = self.activation(x)\n",
    "                x = self.dropout(x)\n",
    "                if self.n_layers > 3:\n",
    "                    x = self.linear4(x)\n",
    "                    x = self.activation(x)\n",
    "                    x = self.dropout(x)\n",
    "                    if self.n_layers > 4:\n",
    "                        x = self.linear5(x)\n",
    "                        x = self.activation(x)\n",
    "                        x = self.dropout(x)\n",
    "                        if self.n_layers > 5:\n",
    "                            x = self.linear6(x)\n",
    "                            x = self.activation(x)\n",
    "                            x = self.dropout(x)\n",
    "                            if self.n_layers > 6:\n",
    "                                x = self.linear7(x)\n",
    "                                x = self.activation(x)\n",
    "                                x = self.dropout(x)\n",
    "                                if self.n_layers > 7:\n",
    "                                    x = self.linear8(x)\n",
    "                                    x = self.activation(x)\n",
    "                                    x = self.dropout(x)\n",
    "                                    if self.n_layers > 8:\n",
    "                                        x = self.linear9(x)\n",
    "                                        x = self.activation(x)\n",
    "                                        x = self.dropout(x)\n",
    "                                        if self.n_layers > 9:\n",
    "                                            x = self.linear10(x)\n",
    "                                            x = self.activation(x)\n",
    "                                            x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        #x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training sample\n",
    "def evaluate(params):\n",
    "    dropout, hidden_size, learning_rate, batch_size, n_hidden = params\n",
    "\n",
    "    max_epochs = 1000\n",
    "    max_patience = 10\n",
    "\n",
    "    best_epochs = []\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    torch.manual_seed(seed)\n",
    "    model = TuneableModel(hidden_size, dropout, n_hidden)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.99, 0.99), weight_decay=1e-2)\n",
    "\n",
    "    dataloaders = k_fold_split_data(dataset, batch_size, k=10)\n",
    "    for i, dataloader in tqdm(enumerate(dataloaders)):\n",
    "        last_loss = 1000000\n",
    "        best_epoch = 0\n",
    "        PATH = f\"model_{i}.pt\"\n",
    "        train_dataloader, validation_dataloader = dataloader[0], dataloader[1]\n",
    "        test_dataloader = dataloader[1]\n",
    "        for epoch in range(max_epochs):\n",
    "            # training\n",
    "            train_loss, train_accuracy, train_f1, train_confusion = train_test(model, train_dataloader, optimizer, training=\"train\")\n",
    "            train_loss, train_accuracy, train_f1 = round(train_loss, 2), round(train_accuracy, 4), round(train_f1, 2)\n",
    "            # validation at end of epoch\n",
    "            validation_loss, validation_accuracy, validation_f1, validation_confusion = train_test(model, validation_dataloader, optimizer, training=\"validation\")\n",
    "            validation_loss, validation_accuracy, validation_f1 = round(validation_loss, 2), round(validation_accuracy, 4), round(validation_f1, 2)\n",
    "            if validation_loss < last_loss:\n",
    "                last_loss = validation_loss\n",
    "                best_epoch = epoch\n",
    "                current_patience = 0\n",
    "            else:\n",
    "                if current_patience == 0:\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': last_loss,\n",
    "                        }, PATH)\n",
    "                current_patience += 1\n",
    "            if current_patience == max_patience:\n",
    "                break   \n",
    "            # if epoch % 100 == 0 and epoch != 0:\n",
    "            #     print(f\"Epoch {epoch}: Train loss: {train_loss}, Train accuracy: {train_accuracy}, Train f1: {train_f1}\")\n",
    "            #     print(f\"Epoch {epoch}: Validation loss: {validation_loss}, Validation accuracy: {validation_accuracy}, Validation f1: {validation_f1}\")\n",
    "\n",
    "        # Testing once patience is reached\n",
    "        torch.manual_seed(seed)\n",
    "        model = TuneableModel(hidden_size, dropout, n_hidden)\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.99, 0.99), weight_decay=1e-4)\n",
    "        checkpoint = torch.load(PATH)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        prediction_list, label_list = train_test(model, test_dataloader, optimizer, training=\"test\")\n",
    "        predictions.extend(prediction_list)\n",
    "        labels.extend(label_list)\n",
    "        best_epochs.append(best_epoch)\n",
    "    print(\"Average training epochs for best model:\", round(np.mean(best_epochs), 1))\n",
    "    print(\"Best epochs:\\n\", best_epochs)\n",
    "    return accuracy_score(labels, predictions), f1_score(labels, predictions), confusion_matrix(labels, predictions)\n",
    "    # print(f\"Average accuracy: {round(np.mean(accuracies), 2)}%\")\n",
    "    # print(f\"Average f1: {round(np.mean(f1s), 2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:10,  1.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training epochs for best model: 1.8\n",
      "Best epochs:\n",
      " [4, 7, 3, 3, 1, 0, 0, 0, 0, 0]\n",
      "acc: 97.67%\n",
      " f1: 0.953\n",
      "[[254   4]\n",
      " [  4  81]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Patience = 6\n",
    "Average training epochs for best model: 1.8\n",
    "Best epochs:\n",
    "    [4, 7, 3, 3, 1, 0, 0, 0, 0, 0]\n",
    "    \n",
    "# 10-fold accuracy 97.67% f1 0.953 \n",
    "\n",
    "Cumulative 10-fold confusion:\n",
    "    [[254   4]\n",
    "    [  4  81]]\n",
    "\n",
    "@ params(\n",
    "    dropout: 0.0\n",
    "    layer size: 500\n",
    "    lr: 0.001\n",
    "    batch_size: 32\n",
    "    n_layers: 5)\n",
    "@ AdamW(\n",
    "    betas=(0.99, 0.99), \n",
    "    weight_decay=1e-2), \n",
    "\"\"\"\n",
    "\n",
    "params = (0.0, 500, 0.001, 32, 5) \n",
    "accuracy, f1, confusion = evaluate(params)\n",
    "print(f\"acc: {round(accuracy*100,2)}%\\n f1: {round(f1,3)}\")\n",
    "print(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_nn ={\n",
    "#     'dropout': [0.5],\n",
    "#     'hidden_size': list(range(500, 501, 100)),\n",
    "#     'learning_rate':[0.01, 0.001, 0.0001, 0.00001],\n",
    "#     'batch_size':[8, 16, 32, 64, 128],\n",
    "#     'n_hidden': list(range(1, 4, 1))\n",
    "# }\n",
    "# parameter_expansion = list(product(*params_nn.values()))\n",
    "# print(len(parameter_expansion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = {}\n",
    "# for i, p in tqdm(enumerate(parameter_expansion)):\n",
    "#     dropout, hidden_size, learning_rate, batch_size, n_hidden = p\n",
    "#     accuracy, f1 = evaluate(p)\n",
    "#     model_performance = {\"dropout\": dropout, \"hidden_size\": hidden_size, \"learning_rate\": learning_rate, \n",
    "#               \"batch_size\": batch_size, \"n_hidden\": n_hidden, \"accuracy\": accuracy, \"f1\": f1}\n",
    "#     results[i] = model_performance\n",
    "#     print(model_performance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_dataframe = pd.DataFrame.from_dict(results, orient='index')\n",
    "# # save results to file\n",
    "# results_dataframe.to_csv(\"preliminary_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
